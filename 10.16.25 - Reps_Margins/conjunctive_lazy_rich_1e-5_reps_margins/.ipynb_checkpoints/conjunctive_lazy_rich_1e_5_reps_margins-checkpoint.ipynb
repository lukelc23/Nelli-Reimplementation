{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0439a28",
   "metadata": {},
   "source": [
    "# Waiting for changes:\n",
    "- os fix for csv\n",
    "\n",
    "- load margins from csv\n",
    "- make h1 and h2 smaller?  (probably unnecessary until later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd040d",
   "metadata": {},
   "source": [
    "# Changes\n",
    "* print out the margin contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5821b02",
   "metadata": {},
   "source": [
    "# Knowledge Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bbe3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nellinetworkconjunctive\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "grandparent_dir = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "greatgrandparent_dir = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..'))\n",
    "sys.path.insert(0, greatgrandparent_dir)\n",
    "sys.path.insert(0, grandparent_dir)\n",
    "sys.path.insert(0, parent_dir)  \n",
    "\n",
    "from nellinetworkconjunctivev2 import Network\n",
    "from helpers import fit_sigmoid, mean_squared_error, rotate, load_behavioural_data\n",
    "from plotting import plotting_init, matrix_plot, mds_plot\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f47dbf",
   "metadata": {},
   "source": [
    "## Load behavioural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc99439f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "../behavioural-data/test-short.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m midd_performance, high_performers, low_performers \u001b[38;5;241m=\u001b[39m load_behavioural_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../behavioural-data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/share/ctn/users/lc3616/Nelli Reimplementation/helpers.py:64\u001b[0m, in \u001b[0;36mload_behavioural_data\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_behavioural_data\u001b[39m(directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load experimental behavioural data\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    :param directory: Directory of behavioural files\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    :return: Behavioural data\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     midd_performance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-short.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m), delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m     high_performers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-long-high-performers.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m), delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m     low_performers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-long-low-performers.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m), delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/share/apps/anaconda3-2023.07.03/lib/python3.11/site-packages/numpy/lib/npyio.py:1356\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1354\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1356\u001b[0m arr \u001b[38;5;241m=\u001b[39m _read(fname, dtype\u001b[38;5;241m=\u001b[39mdtype, comment\u001b[38;5;241m=\u001b[39mcomment, delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[1;32m   1357\u001b[0m             converters\u001b[38;5;241m=\u001b[39mconverters, skiplines\u001b[38;5;241m=\u001b[39mskiprows, usecols\u001b[38;5;241m=\u001b[39musecols,\n\u001b[1;32m   1358\u001b[0m             unpack\u001b[38;5;241m=\u001b[39munpack, ndmin\u001b[38;5;241m=\u001b[39mndmin, encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   1359\u001b[0m             max_rows\u001b[38;5;241m=\u001b[39mmax_rows, quote\u001b[38;5;241m=\u001b[39mquotechar)\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m/share/apps/anaconda3-2023.07.03/lib/python3.11/site-packages/numpy/lib/npyio.py:975\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    973\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 975\u001b[0m     fh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m_datasource\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/share/apps/anaconda3-2023.07.03/lib/python3.11/site-packages/numpy/lib/_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mopen(path, mode, encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n",
      "File \u001b[0;32m/share/apps/anaconda3-2023.07.03/lib/python3.11/site-packages/numpy/lib/_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ../behavioural-data/test-short.txt not found."
     ]
    }
   ],
   "source": [
    "midd_performance, high_performers, low_performers = load_behavioural_data(\"../behavioural-data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b926b99",
   "metadata": {},
   "source": [
    "## Set simulation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"main\" # \"lazy-regime\", \"two-readouts\", \"fixed-inputs\"\n",
    "mode2 = \"main\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adce2e",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seeds\n",
    "seeds_n = 10\n",
    "mds_seed = 1\n",
    "\n",
    "# Number of input items\n",
    "items_n = 7\n",
    "\n",
    "# Experiment details\n",
    "training_blocks = 1\n",
    "trials = 3000\n",
    "training_length = training_blocks * trials\n",
    "# stitching_steps = 20\n",
    "\n",
    "# Network hyperparameters\n",
    "readouts = 2 if mode == \"two-readouts\" else 1\n",
    "h1_size = 20\n",
    "\n",
    "if mode == \"lazy-regime\":\n",
    "    w1_weight_std = np.sqrt(6. / items_n)\n",
    "    w2_weight_std = np.sqrt(10. / h1_size)\n",
    "else:\n",
    "    w1_weight_std = 0.025 * np.sqrt(1 / items_n)\n",
    "    w2_weight_std = np.sqrt(1 / h1_size)\n",
    "\n",
    "proportion = 1e5    \n",
    "    \n",
    "if mode2 == \"lazy-regime\":\n",
    "    w3_weight_std = np.sqrt(6. / items_n)\n",
    "    w4_weight_std = np.sqrt(10. / h1_size)\n",
    "else:\n",
    "    w3_weight_std = 0.025 * np.sqrt(1 / items_n) / proportion\n",
    "    w4_weight_std = np.sqrt(1 / h1_size) / proportion\n",
    "\n",
    "learning_rate = 0.03 if mode == \"lazy-regime\" else 0.05\n",
    "learning_rate_layers_3_4 = 0.05\n",
    "\n",
    "# Parameter space for gridsearch\n",
    "gammas = np.concatenate([[0], np.geomspace(1e-4, 1, 69)])\n",
    "ss = np.geomspace(1e-2, 100, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d644ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w1_weight_std)\n",
    "print(w3_weight_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b045f1c",
   "metadata": {},
   "source": [
    "# TI Exp with conjunctive population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_networks_exp(gamma):\n",
    "    # Log\n",
    "    results = {\n",
    "        \"train\": {\n",
    "            \"losses\": np.zeros((seeds_n, training_length)),\n",
    "            \"w1s\": np.zeros((seeds_n, training_length, h1_size, items_n)),\n",
    "            \"w2s\": np.zeros((seeds_n, training_length, readouts, h1_size)),\n",
    "            \"h1s\": np.zeros((seeds_n, training_length, items_n, h1_size)),\n",
    "            \"h2s\": np.zeros((seeds_n, training_length, items_n, h1_size)),\n",
    "            \"r1s\": np.zeros((seeds_n, training_length, items_n, h1_size)),\n",
    "            \"r2s\": np.zeros((seeds_n, training_length, items_n, h1_size)), \n",
    "            \"certainties\": np.zeros((seeds_n, training_length, items_n, items_n)),\n",
    "            \"evals\": np.zeros((seeds_n, items_n, items_n)),\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    results[\"train\"][\"training_progress\"] = np.zeros((seeds_n, training_length, items_n, items_n))\n",
    "    \n",
    "    for seed in range(seeds_n):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Init Network\n",
    "        model = Network(items_n, h1_size, w1_weight_std, w2_weight_std, w3_weight_std, w4_weight_std, readouts=readouts)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimiser = torch.optim.SGD([\n",
    "            {'params': [*model.layer_1.parameters(), *model.layer_2.parameters()], 'lr': learning_rate},\n",
    "            {'params': [*model.layer_3.parameters(), *model.layer_4.parameters()], 'lr': learning_rate_layers_3_4}\n",
    "        ])\n",
    "\n",
    "        training_step = 0\n",
    "        items_per_context = items_n\n",
    "        \n",
    "        for block in range(training_blocks): \n",
    "            items_per_context = 7\n",
    "            p = 4\n",
    "            q = 2\n",
    "            training_pairs_norm = np.asarray(list(zip(range(0, items_per_context - 1), range(1, items_per_context))))\n",
    "            training_pairs_exp = np.asarray([[p,q]])\n",
    "            training_pairs = np.concatenate([training_pairs_norm, training_pairs_exp], axis=0)\n",
    "            for trial in range(trials):\n",
    "                # Sample input and target\n",
    "                random_index = np.random.randint(0, len(training_pairs))\n",
    "                item_1, item_2 = np.random.choice(training_pairs[random_index], 2, False)\n",
    "                if readouts == 1:\n",
    "                    if item_1 == p and item_2 == q or item_1 == q and item_2 == p:\n",
    "                        exception = True\n",
    "                    else:\n",
    "                        exception = False\n",
    "                    if not exception:\n",
    "                        target = torch.tensor([1. if item_1 > item_2 else -1.])\n",
    "                    else:\n",
    "                        target = torch.tensor([-1. if item_1 > item_2 else 1.])\n",
    "\n",
    "                #UNCHANGED SO FAR\n",
    "                elif readouts == 2:\n",
    "                    target = torch.tensor([1., -1.] if item_1 > item_2 else [-1., 1.])\n",
    "\n",
    "\n",
    "                # Forward propagate and backpropagate\n",
    "                optimiser.zero_grad()\n",
    "                _, output = model(item_1, item_2)\n",
    "                model.loss = criterion(output, target)\n",
    "                model.loss.backward()\n",
    "                model.correct(learning_rate, gamma)\n",
    "                optimiser.step()\n",
    "\n",
    "                # Log\n",
    "                with torch.no_grad():\n",
    "                    results[\"train\"][\"losses\"][seed, training_step] = model.loss.item()\n",
    "                    results[\"train\"][\"w1s\"][seed, training_step] = model.layer_1.weight.detach().numpy().copy()\n",
    "                    results[\"train\"][\"w2s\"][seed, training_step] = model.layer_2.weight.detach().numpy().copy()\n",
    "                    results[\"train\"][\"h1s\"][seed, training_step] = model.extract_h1s()\n",
    "                    results[\"train\"][\"h2s\"][seed, training_step] = model.extract_h2s()\n",
    "                    results[\"train\"][\"h1s\"][seed, training_step] = model.extract_r1s()\n",
    "                    results[\"train\"][\"h2s\"][seed, training_step] = model.extract_r2s()\n",
    "                    results[\"train\"][\"certainties\"][seed, training_step] = model.pairwise_certainty.a.copy()\n",
    "                    results[\"train\"][\"training_progress\"][seed, training_step] = model.evaluate()\n",
    "                \n",
    "                training_step += 1\n",
    "        \n",
    "        # Evaluate\n",
    "        with torch.no_grad():\n",
    "            results[\"train\"][\"evals\"][seed] = model.evaluate()\n",
    "        \n",
    "    return gamma, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_networks_exp(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a8c48",
   "metadata": {},
   "source": [
    "# Check the convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the average loss across all seeds during training\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "losses = results[\"train\"][\"losses\"]  # shape: (seeds_n, training_steps)\n",
    "avg_loss = losses.mean(axis=0)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(avg_loss, label='Average Loss')\n",
    "plt.xlabel(\"Training Time Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Average Training Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abae4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for every possible pair (i, j) in different figures\n",
    "# If results is a tuple, unpack it to get the actual results dict\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "training_progress = results[\"train\"][\"training_progress\"]\n",
    "seeds_n = training_progress.shape[0]\n",
    "items_n = training_progress.shape[2]\n",
    "\n",
    "for i in range(items_n):\n",
    "    for j in range(items_n):\n",
    "        if i == j:\n",
    "            continue  # skip diagonal if desired\n",
    "        plt.figure(figsize=(7,4))\n",
    "        for seed in range(seeds_n):\n",
    "            plt.plot(training_progress[seed, :480, i, j], alpha=0.5)\n",
    "        plt.xlabel(\"Training Time Steps\")\n",
    "        plt.ylabel(\"Margin\")\n",
    "        plt.title(f\"Pair ({i}, {j})\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc529888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average margin across seeds and time for each (i, j) pair\n",
    "# Resulting in a grid of shape (items_n, items_n)\n",
    "t_final = training_progress.shape[1] - 1\n",
    "average_margin_grid = training_progress[:, t_final, :, :].mean(axis=0)\n",
    "print(\"Average margin grid (across seeds and final time):\")\n",
    "print(average_margin_grid)\n",
    "\n",
    "# Plot the average margin grid as a heatmap with values over each rectangle\n",
    "plt.figure(figsize=(6, 5))\n",
    "im = plt.imshow(average_margin_grid, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(im, label=\"Average Margin\")\n",
    "plt.xlabel(\"j\")\n",
    "plt.ylabel(\"i\")\n",
    "plt.title(\"Average Margin Grid Across Seeds and Time\")\n",
    "\n",
    "# Add values in each cell\n",
    "for i in range(average_margin_grid.shape[0]):\n",
    "    for j in range(average_margin_grid.shape[1]):\n",
    "        value = average_margin_grid[i, j]\n",
    "        plt.text(j, i, f\"{value:.2f}\", ha='center', va='center', color='white' if im.norm(value) < 0.5 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce42c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"correct\" average margin grid where (4,2) and (2,4) entries are multiplied by -1, as well as if i>j\n",
    "correct_margin_grid = average_margin_grid.copy()\n",
    "correct_margin_grid[4, 2] *= -1\n",
    "correct_margin_grid[2, 4] *= -1\n",
    "for i in range(correct_margin_grid.shape[0]):\n",
    "    for j in range(correct_margin_grid.shape[1]):\n",
    "        if i>j and ( (i != 2 and j != 4) or (i != 4 and j != 2) ):\n",
    "            correct_margin_grid[i,j] *= -1\n",
    "\n",
    "print(\"Corrected average margin grid:\")\n",
    "print(correct_margin_grid)\n",
    "\n",
    "# Plot the corrected average margin grid as a heatmap with values over each rectangle\n",
    "plt.figure(figsize=(6, 5))\n",
    "im_correct = plt.imshow(correct_margin_grid, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(im_correct, label=\"Corrected Average Margin\")\n",
    "plt.xlabel(\"j\")\n",
    "plt.ylabel(\"i\")\n",
    "plt.suptitle(\"Corrected Average Margin Grid (Flipped (4,2) and (2,4))\")\n",
    "plt.title(\"conjunctive_small_lr copy\")\n",
    "\n",
    "# Add values in each cell\n",
    "for i in range(correct_margin_grid.shape[0]):\n",
    "    for j in range(correct_margin_grid.shape[1]):\n",
    "        value = correct_margin_grid[i, j]\n",
    "        plt.text(j, i, f\"{value:.2f}\", ha='center', va='center', color='white' if im_correct.norm(value) < 0.5 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sigmoid of the corrected average margin grid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "sigmoid_correct_margin_grid = sigmoid(correct_margin_grid)\n",
    "\n",
    "print(\"Sigmoid of corrected average margin grid:\")\n",
    "print(sigmoid_correct_margin_grid)\n",
    "\n",
    "# Plot the sigmoid margin grid as a heatmap with values in each cell\n",
    "plt.figure(figsize=(6, 5))\n",
    "im_sigmoid_correct = plt.imshow(sigmoid_correct_margin_grid, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(im_sigmoid_correct, label=\"Sigmoid(Corrected Average Margin)\")\n",
    "plt.xlabel(\"j\")\n",
    "plt.ylabel(\"i\")\n",
    "plt.suptitle(\"Sigmoid of Corrected Average Margin Grid\")\n",
    "plt.title(\"conjunctive_small_lr_copy\", color='gray')\n",
    "\n",
    "# Add values in each cell\n",
    "for i in range(sigmoid_correct_margin_grid.shape[0]):\n",
    "    for j in range(sigmoid_correct_margin_grid.shape[1]):\n",
    "        value = sigmoid_correct_margin_grid[i, j]\n",
    "        plt.text(j, i, f\"{value:.2f}\", ha='center', va='center', color='white' if im_sigmoid_correct.norm(value) < 0.5 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae7ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set your training pair (ensure p, q are defined as desired)\n",
    "p, q = 4, 2\n",
    "\n",
    "items_n = correct_margin_grid.shape[0]\n",
    "\n",
    "# Masks: only (p,q) and (q,p) are training; test = all other off-diagonals\n",
    "non_diag = ~np.eye(items_n, dtype=bool)\n",
    "train_mask = np.zeros((items_n, items_n), dtype=bool)\n",
    "train_mask[p, q] = True\n",
    "train_mask[q, p] = True\n",
    "test_mask = non_diag & ~train_mask\n",
    "\n",
    "train_vals = correct_margin_grid[train_mask]\n",
    "test_vals = correct_margin_grid[test_mask]\n",
    "\n",
    "# Jittered x-positions for scatter\n",
    "rng = np.random.default_rng(0)\n",
    "x_train = rng.normal(loc=0, scale=0.05, size=train_vals.size)\n",
    "x_test  = rng.normal(loc=1, scale=0.05, size=test_vals.size)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(x_train, train_vals, color='C0', alpha=0.9, label='Train')\n",
    "plt.scatter(x_test,  test_vals,  color='C1', alpha=0.6, label='Test')\n",
    "\n",
    "# Optional: overlay means\n",
    "plt.hlines(train_vals.mean(), -0.2, 0.2, colors='C0', linestyles='--', linewidth=2)\n",
    "plt.hlines(test_vals.mean(),  0.8,  1.2, colors='C1', linestyles='--', linewidth=2)\n",
    "\n",
    "plt.xticks([0, 1], ['Train', 'Test'])\n",
    "plt.ylabel('Average margin')\n",
    "plt.title(f'Average Pair Margins at Final Time Step (Train: ({p},{q})/({q},{p}))')\n",
    "plt.axhline(0, color='k', linewidth=1, alpha=0.3)\n",
    "plt.xlim(-0.4, 1.4)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter of TEST pair margins grouped by symbolic distance |i - j|\n",
    "# Assumes: correct_margin_grid, t_final, p, q are defined as above\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "items_n = correct_margin_grid.shape[0]\n",
    "I, J = np.indices((items_n, items_n))\n",
    "dist_mat = np.abs(I - J)\n",
    "\n",
    "# Exclude diagonal and training pair (p,q)/(q,p)\n",
    "non_diag = ~np.eye(items_n, dtype=bool)\n",
    "train_mask = np.zeros((items_n, items_n), dtype=bool)\n",
    "train_mask[p, q] = True\n",
    "train_mask[q, p] = True\n",
    "test_mask = non_diag & ~train_mask\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "for d in range(1, items_n):\n",
    "    vals = correct_margin_grid[(dist_mat == d) & test_mask]\n",
    "    if vals.size == 0:\n",
    "        continue\n",
    "    x = rng.normal(loc=d, scale=0.06, size=vals.size)  # jitter around integer distance\n",
    "    plt.scatter(x, vals, alpha=0.6, s=18, label=None)\n",
    "    mean_d = vals.mean()\n",
    "    plt.hlines(mean_d, d - 0.25, d + 0.25, colors='k', linestyles='--', linewidth=1)\n",
    "\n",
    "plt.xticks(range(1, items_n), [str(d) for d in range(1, items_n)])\n",
    "plt.xlabel('Symbolic distance |i - j|')\n",
    "plt.ylabel('Corrected average margin (t = %d)' % t_final)\n",
    "plt.title(f'Test Pair Margins by Symbolic Distance (Train excluded: ({p},{q})/({q},{p}))')\n",
    "plt.axhline(0, color='k', linewidth=1, alpha=0.3)\n",
    "plt.xlim(0.5, items_n - 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter of TEST pair margins grouped by symbolic distance |i - j|, including sd=0\n",
    "# Assumes: correct_margin_grid, t_final, p, q are defined\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "items_n = correct_margin_grid.shape[0]\n",
    "I, J = np.indices((items_n, items_n))\n",
    "dist_mat = np.abs(I - J)\n",
    "\n",
    "# Training = all sd==1 pairs + the special pair (p,q)/(q,p)\n",
    "training_mask = (dist_mat == 1)\n",
    "training_mask[p, q] = True\n",
    "training_mask[q, p] = True\n",
    "\n",
    "# Test = everything else (including diagonal sd=0)\n",
    "test_mask = ~training_mask\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "for d in range(0, items_n):\n",
    "    vals = correct_margin_grid[(dist_mat == d) & test_mask]\n",
    "    if vals.size == 0:\n",
    "        continue\n",
    "    x = rng.normal(loc=d, scale=0.06, size=vals.size)  # jitter around integer distance\n",
    "    plt.scatter(x, vals, alpha=0.6, s=18)\n",
    "    mean_d = vals.mean()\n",
    "    plt.hlines(mean_d, d - 0.25, d + 0.25, colors='k', linestyles='--', linewidth=1)\n",
    "\n",
    "plt.xticks(range(0, items_n), [str(d) for d in range(0, items_n)])\n",
    "plt.xlabel('Symbolic distance |i - j| (test only, includes sd=0)')\n",
    "plt.ylabel('Corrected average margin (t = %d)' % t_final)\n",
    "plt.title(f'Test Pair Margins by Symbolic Distance (Train: sd=1 and ({p},{q})/({q},{p}) excluded)')\n",
    "plt.axhline(0, color='k', linewidth=1, alpha=0.3)\n",
    "plt.xlim(-0.5, items_n - 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter of pair margins by symbolic distance |i - j| (including sd=0),\n",
    "# highlighting the exception pair (p,q)/(q,p) with a distinct color and label.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "items_n = correct_margin_grid.shape[0]\n",
    "I, J = np.indices((items_n, items_n))\n",
    "dist_mat = np.abs(I - J)\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Exclude the exception pair from the base scatter to avoid duplicate markers\n",
    "base_mask = np.ones_like(dist_mat, dtype=bool)\n",
    "base_mask[p, q] = False\n",
    "base_mask[q, p] = False\n",
    "\n",
    "# Plot all pairs grouped by symbolic distance (including sd=0 and sd=1)\n",
    "for d in range(0, items_n):\n",
    "    vals = correct_margin_grid[(dist_mat == d) & base_mask]\n",
    "    if vals.size == 0:\n",
    "        continue\n",
    "    x = rng.normal(loc=d, scale=0.06, size=vals.size)  # jitter around integer distance\n",
    "    plt.scatter(x, vals, alpha=0.6, s=18, color='C0')\n",
    "    mean_d = vals.mean()\n",
    "    plt.hlines(mean_d, d - 0.25, d + 0.25, colors='k', linestyles='--', linewidth=1)\n",
    "\n",
    "# Highlight the exception pair (p,q) and (q,p) at their symbolic distance\n",
    "d_exc = abs(p - q)\n",
    "y_pq = correct_margin_grid[p, q]\n",
    "y_qp = correct_margin_grid[q, p]\n",
    "plt.scatter(d_exc - 0.12, y_pq, color='crimson', marker='*', s=140, zorder=5, label=f'Exception ({p},{q})')\n",
    "plt.scatter(d_exc + 0.12, y_qp, color='crimson', marker='*', s=140, zorder=5)\n",
    "\n",
    "# Annotate the exception points\n",
    "plt.annotate(f'({p},{q})', (d_exc - 0.12, y_pq), textcoords='offset points', xytext=(0, 8),\n",
    "             ha='center', color='crimson', fontsize=9)\n",
    "plt.annotate(f'({q},{p})', (d_exc + 0.12, y_qp), textcoords='offset points', xytext=(0, 8),\n",
    "             ha='center', color='crimson', fontsize=9)\n",
    "\n",
    "plt.xticks(range(0, items_n), [str(d) for d in range(0, items_n)])\n",
    "plt.xlabel('Symbolic distance |i - j| (all pairs)')\n",
    "plt.ylabel('Corrected average margin (t = %d)' % t_final)\n",
    "plt.title('Pair Margins by Symbolic Distance (exception highlighted)')\n",
    "plt.axhline(0, color='k', linewidth=1, alpha=0.3)\n",
    "plt.xlim(-0.5, items_n - 0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter of pair margins by symbolic distance |i - j| (including sd=0),\n",
    "# highlighting the exception pair (p,q)/(q,p) using sigmoid_correct_margin_grid.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "items_n = sigmoid_correct_margin_grid.shape[0]\n",
    "I, J = np.indices((items_n, items_n))\n",
    "dist_mat = np.abs(I - J)\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Exclude the exception pair from the base scatter to avoid duplicate markers\n",
    "base_mask = np.ones_like(dist_mat, dtype=bool)\n",
    "base_mask[p, q] = False\n",
    "base_mask[q, p] = False\n",
    "\n",
    "# Plot all pairs grouped by symbolic distance (including sd=0 and sd=1)\n",
    "for d in range(0, items_n):\n",
    "    vals = sigmoid_correct_margin_grid[(dist_mat == d) & base_mask]\n",
    "    if vals.size == 0:\n",
    "        continue\n",
    "    x = rng.normal(loc=d, scale=0.06, size=vals.size)  # jitter around integer distance\n",
    "    plt.scatter(x, vals, alpha=0.6, s=18, color='C0')\n",
    "    mean_d = vals.mean()\n",
    "    plt.hlines(mean_d, d - 0.25, d + 0.25, colors='k', linestyles='--', linewidth=1)\n",
    "\n",
    "# Highlight the exception pair (p,q) and (q,p) at their symbolic distance\n",
    "d_exc = abs(p - q)\n",
    "y_pq = sigmoid_correct_margin_grid[p, q]\n",
    "y_qp = sigmoid_correct_margin_grid[q, p]\n",
    "plt.scatter(d_exc - 0.12, y_pq, color='crimson', marker='*', s=140, zorder=5, label=f'Exception ({p},{q})')\n",
    "plt.scatter(d_exc + 0.12, y_qp, color='crimson', marker='*', s=140, zorder=5)\n",
    "\n",
    "# Annotate the exception points\n",
    "plt.annotate(f'({p},{q})', (d_exc - 0.12, y_pq), textcoords='offset points', xytext=(0, 8),\n",
    "             ha='center', color='crimson', fontsize=9)\n",
    "plt.annotate(f'({q},{p})', (d_exc + 0.12, y_qp), textcoords='offset points', xytext=(0, 8),\n",
    "             ha='center', color='crimson', fontsize=9)\n",
    "\n",
    "plt.xticks(range(0, items_n), [str(d) for d in range(0, items_n)])\n",
    "plt.xlabel('Symbolic distance |i - j| (all pairs)')\n",
    "plt.ylabel('Sigmoid-corrected average margin (t = %d)' % t_final)\n",
    "plt.title('Pair Margins by Symbolic Distance (exception highlighted)')\n",
    "plt.axhline(0, color='k', linewidth=1, alpha=0.3)\n",
    "plt.xlim(-0.5, items_n - 0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Extract training pair results and save to CSV\n",
    "# We'll save the average margin over seeds for each pair (i, j) at each time step\n",
    "# Additionally, save h1s and h2s activations (per time_step, item, unit) with per-seed values, mean, and std\n",
    "\n",
    "# If results is a tuple, unpack it to get the actual results dict\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "train_dict = results[\"train\"]\n",
    "\n",
    "# Margins (training_progress): shape (seeds_n, time_steps, items_n, items_n)\n",
    "training_progress = train_dict[\"training_progress\"]\n",
    "seeds_n, time_steps, items_n, _ = training_progress.shape\n",
    "\n",
    "# Prepare CSV header for margins\n",
    "header_margins = [\"time_step\", \"i\", \"j\"] + [f\"seed_{seed}\" for seed in range(seeds_n)] + [\"mean_margin\", \"std_margin\"]\n",
    "\n",
    "rows = []\n",
    "for i in range(items_n):\n",
    "    for j in range(items_n):\n",
    "        for t in range(time_steps):\n",
    "            margins = training_progress[:, t, i, j]\n",
    "            mean_margin = margins.mean()\n",
    "            std_margin = margins.std()\n",
    "            row = [t, i, j] + list(margins) + [mean_margin, std_margin]\n",
    "            rows.append(row)\n",
    "\n",
    "# Write margins CSV\n",
    "csv_filename = f\"conjunctive_lazy_rich.csv\"\n",
    "with open(csv_filename, mode=\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header_margins)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "# Gzip the margins CSV\n",
    "csv_gz_filename = csv_filename + \".gz\"\n",
    "with open(csv_filename, 'rb') as f_in, gzip.open(csv_gz_filename, 'wb') as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(f\"Training pair results saved to {csv_filename} (gzipped to {csv_gz_filename})\")\n",
    "\n",
    "# Save h1s if available: shape (seeds_n, time_steps, items_n, h1_size)\n",
    "if \"h1s\" in train_dict and train_dict[\"h1s\"] is not None:\n",
    "    h1s = train_dict[\"h1s\"]\n",
    "    h1_seeds, h1_time_steps, h1_items_n, h1_size = h1s.shape\n",
    "    assert h1_seeds == seeds_n and h1_time_steps == time_steps and h1_items_n == items_n, \"h1s shape mismatch\"\n",
    "\n",
    "    header_h1 = [\"time_step\", \"item\", \"unit\"] + [f\"seed_{seed}\" for seed in range(seeds_n)] + [\"mean_activation\", \"std_activation\"]\n",
    "    rows_h1 = []\n",
    "    for t in range(time_steps):\n",
    "        for i in range(items_n):\n",
    "            for unit in range(h1_size):\n",
    "                vals = h1s[:, t, i, unit]\n",
    "                rows_h1.append([t, i, unit] + list(vals) + [vals.mean(), vals.std()])\n",
    "\n",
    "    h1_csv_filename = f\"conjunctive_lazy_rich_h1s.csv\"\n",
    "    with open(h1_csv_filename, mode=\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header_h1)\n",
    "        writer.writerows(rows_h1)\n",
    "    # Gzip the h1s CSV\n",
    "    h1_csv_gz_filename = h1_csv_filename + \".gz\"\n",
    "    with open(h1_csv_filename, 'rb') as f_in, gzip.open(h1_csv_gz_filename, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    print(f\"h1 activations saved to {h1_csv_filename} (gzipped to {h1_csv_gz_filename})\")\n",
    "else:\n",
    "    print(\"h1s not found in results['train']; skipping h1 CSV.\")\n",
    "\n",
    "# Save h2s if available: shape (seeds_n, time_steps, items_n, h2_size)\n",
    "if \"h2s\" in train_dict and train_dict[\"h2s\"] is not None:\n",
    "    h2s = train_dict[\"h2s\"]\n",
    "    h2_seeds, h2_time_steps, h2_items_n, h2_size = h2s.shape\n",
    "    if h2_seeds == seeds_n and h2_time_steps == time_steps and h2_items_n == items_n:\n",
    "        header_h2 = [\"time_step\", \"item\", \"unit\"] + [f\"seed_{seed}\" for seed in range(seeds_n)] + [\"mean_activation\", \"std_activation\"]\n",
    "        rows_h2 = []\n",
    "        for t in range(time_steps):\n",
    "            for i in range(items_n):\n",
    "                for unit in range(h2_size):\n",
    "                    vals = h2s[:, t, i, unit]\n",
    "                    rows_h2.append([t, i, unit] + list(vals) + [vals.mean(), vals.std()])\n",
    "\n",
    "        h2_csv_filename = f\"conjunctive_lazy_rich_h2s.csv\"\n",
    "        with open(h2_csv_filename, mode=\"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header_h2)\n",
    "            writer.writerows(rows_h2)\n",
    "        # Gzip the h2s CSV\n",
    "        h2_csv_gz_filename = h2_csv_filename + \".gz\"\n",
    "        with open(h2_csv_filename, 'rb') as f_in, gzip.open(h2_csv_gz_filename, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"h2 activations saved to {h2_csv_filename} (gzipped to {h2_csv_gz_filename})\")\n",
    "    else:\n",
    "        print(\"h2s shape mismatch with training_progress; skipping h2 CSV.\")\n",
    "else:\n",
    "    print(\"h2s not found in results['train']; skipping h2 CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a87896",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = f\"conjunctive_lazy_rich.csv\"\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file with training pair margins\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Display the first few rows to verify extraction\n",
    "print(\"Extracted results from CSV:\")\n",
    "print(df.head())\n",
    "\n",
    "# Now 'df' contains the extracted results and can be used for further analysis or plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff32dea",
   "metadata": {},
   "source": [
    "Figure out keyerror on t issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadf932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
    "\n",
    "# Try to find the correct time, i, j column names\n",
    "# The original code expects 't', but the CSV uses 'time_step'\n",
    "# Let's map accordingly\n",
    "col_map = {}\n",
    "for col in df.columns:\n",
    "    if col.lower() in ['t', 'time', 'time_step']:\n",
    "        col_map['t'] = col\n",
    "    if col.lower() == 'i':\n",
    "        col_map['i'] = col\n",
    "    if col.lower() == 'j':\n",
    "        col_map['j'] = col\n",
    "\n",
    "# Use mapped column names\n",
    "t_col = col_map.get('t', 'time_step')\n",
    "i_col = col_map.get('i', 'i')\n",
    "j_col = col_map.get('j', 'j')\n",
    "\n",
    "df_sorted = df.sort_values([t_col, i_col, j_col]).reset_index(drop=True)\n",
    "\n",
    "# The number of margin columns is seeds_n\n",
    "margin_cols = [col for col in df.columns if col not in [t_col, i_col, j_col, 'mean_margin', 'std_margin']]\n",
    "seeds_n = len(margin_cols)\n",
    "time_steps = df[t_col].max() + 1  # assumes t is 0-based and contiguous\n",
    "items_n = max(df[i_col].max(), df[j_col].max()) + 1  # assuming 0-based indexing\n",
    "\n",
    "# Initialize training_progress array\n",
    "training_progress = np.zeros((seeds_n, time_steps, items_n, items_n))\n",
    "\n",
    "# Fill the array\n",
    "for idx, row in df.iterrows():\n",
    "    t = int(row[t_col])\n",
    "    i = int(row[i_col])\n",
    "    j = int(row[j_col])\n",
    "    for s, col in enumerate(margin_cols):\n",
    "        training_progress[s, t, i, j] = row[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b482789",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_per_context = 7\n",
    "p = 4\n",
    "q = 2\n",
    "\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "n = items_n\n",
    "Grid = np.zeros((seeds_n, n, n))\n",
    "for seed in range(seeds_n):\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            Grid[seed, i, j] = training_progress[seed, training_progress.shape[1]-1, i, j]\n",
    "Average_Grid = np.mean(Grid, axis=0)\n",
    "Std_Grid = np.var(Grid, axis=0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(Average_Grid, cmap='viridis', interpolation='none')\n",
    "plt.colorbar(label='Mean Margin')\n",
    "plt.title(f'Mean Margin Grid for TI exp on Nelli Conjunctive Network\\n(items_n={items_n}, exception=({p},{q}))')\n",
    "plt.xlabel('Item i')\n",
    "plt.ylabel('Item j')\n",
    "plt.xticks(range(n))\n",
    "plt.yticks(range(n))\n",
    "\n",
    "# Plot mean numbers over each square\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        plt.text(j, i, f\"{Average_Grid[i, j]:.2f}\", ha='center', va='center', color='white' if Average_Grid[i, j] < (Average_Grid.max() / 2) else 'black', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(Std_Grid, cmap='magma', interpolation='none')\n",
    "plt.colorbar(label='Variance of Margin')\n",
    "plt.title(f'Variance Margin Grid for TI exp on Nelli Conjunctive Network\\n(items_n={items_n}, exception=({p},{q}))')\n",
    "plt.xlabel('Item i')\n",
    "plt.ylabel('Item j')\n",
    "plt.xticks(range(n))\n",
    "plt.yticks(range(n))\n",
    "# Plot variance numbers over each square\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        plt.text(j, i, f\"{Std_Grid[i, j]:.2f}\", ha='center', va='center', color='white' if Average_Grid[i, j] < (Average_Grid.max() / 2) else 'black', fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108aa523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if isinstance(results, tuple):\n",
    "    # results = results[1]\n",
    "# training_progress = results[\"train\"][\"training_progress\"]\n",
    "n = items_n\n",
    "Grid = np.zeros((seeds_n, n, n))\n",
    "for seed in range(seeds_n):\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            Grid[seed, i, j] = training_progress[seed, training_length - 1, i, j]\n",
    "Average_Grid = np.mean(Grid, axis=0)\n",
    "# Apply sigmoid function to Average_Grid\n",
    "Average_Grid = 1 / (1 + np.exp(-Average_Grid))\n",
    "Std_Grid = np.var(Grid, axis=0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(Average_Grid, cmap='viridis', interpolation='none')\n",
    "plt.colorbar(label='Mean Margin')\n",
    "plt.title(f'Mean Sigmoid(Margin) Grid for TI exp on Nelli Conjunctive Network\\n(items_n={items_n}, exception=({p},{q}))')\n",
    "plt.xlabel('Item i')\n",
    "plt.ylabel('Item j')\n",
    "plt.xticks(range(n))\n",
    "plt.yticks(range(n))\n",
    "# Plot mean numbers over each square\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        plt.text(j, i, f\"{Average_Grid[i, j]:.2f}\", ha='center', va='center', color='white' if Average_Grid[i, j] < (Average_Grid.max() / 2) else 'black', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(Std_Grid, cmap='magma', interpolation='none')\n",
    "plt.colorbar(label='Variance of Margin')\n",
    "plt.title('Variance Grid for TI exp on Nelli Conjunctive Network')\n",
    "plt.xlabel('Item i')\n",
    "plt.ylabel('Item j')\n",
    "plt.xticks(range(n))\n",
    "plt.yticks(range(n))\n",
    "# Plot variance numbers over each square\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        plt.text(j, i, f\"{Std_Grid[i, j]:.2f}\", ha='center', va='center', color='white' if Average_Grid[i, j] < (Average_Grid.max() / 2) else 'black', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb978f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each non-exception item vs each exception item\n",
    "# Exception items: 2, 3, 4 (from p=4, q=2)\n",
    "# Non-exception items: 0, 1, 5, 6\n",
    "\n",
    "# Import MarginExtractor from parent directory\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the parent directory (where csv_margin.py should be)\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# If csv_margin.py is not in parent_dir, try current directory or set explicit path\n",
    "if not os.path.exists(os.path.join(parent_dir, 'csv_margin.py')):\n",
    "    # Check if it's in the current directory\n",
    "    if os.path.exists(os.path.join(current_dir, 'csv_margin.py')):\n",
    "        parent_dir = current_dir\n",
    "    else:\n",
    "        # Explicitly set to the Nelli Reimplementation directory\n",
    "        parent_dir = r'Z:\\Luke\\Nelli Reimplementation'\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Using directory: {parent_dir}\")\n",
    "print(f\"csv_margin.py exists: {os.path.exists(os.path.join(parent_dir, 'csv_margin.py'))}\")\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "    \n",
    "from csv_margin import MarginExtractor\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file that was already created\n",
    "csv_filename = \"conjunctive_lazy_rich.csv\"\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Reconstruct training_progress from CSV (same as Cell 27)\n",
    "t_col = 'time_step'\n",
    "i_col = 'i'\n",
    "j_col = 'j'\n",
    "\n",
    "margin_cols = [col for col in df.columns if col not in [t_col, i_col, j_col, 'mean_margin', 'std_margin']]\n",
    "csv_seeds_n = len(margin_cols)\n",
    "csv_time_steps = df[t_col].max() + 1\n",
    "csv_items_n = max(df[i_col].max(), df[j_col].max()) + 1\n",
    "\n",
    "# Initialize training_progress array from CSV\n",
    "training_progress_from_csv = np.zeros((csv_seeds_n, csv_time_steps, csv_items_n, csv_items_n))\n",
    "\n",
    "# Fill the array\n",
    "for idx, row in df.iterrows():\n",
    "    t = int(row[t_col])\n",
    "    i = int(row[i_col])\n",
    "    j = int(row[j_col])\n",
    "    for s, col in enumerate(margin_cols):\n",
    "        training_progress_from_csv[s, t, i, j] = row[col]\n",
    "\n",
    "# Create a mock results dict for MarginExtractor\n",
    "results_from_csv = {\n",
    "    \"train\": {\n",
    "        \"training_progress\": training_progress_from_csv\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create MarginExtractor from CSV data\n",
    "extractor = MarginExtractor(results_from_csv)\n",
    "print(f\"Loaded data from CSV: {csv_seeds_n} seeds, {csv_time_steps} timesteps, {csv_items_n} items\")\n",
    "\n",
    "# Define exception and non-exception items\n",
    "p, q = 4, 2\n",
    "exception_items = list(range(min(p, q), max(p, q) + 1))  # [2, 3, 4]\n",
    "all_items = list(range(items_n))\n",
    "non_exception_items = [i for i in all_items if i not in exception_items]\n",
    "\n",
    "print(f\"Exception items: {exception_items}\")\n",
    "print(f\"Non-exception items: {non_exception_items}\")\n",
    "\n",
    "# Plot each non-exception item against each exception item\n",
    "for non_exc in non_exception_items:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    fig.suptitle(f\"Item {non_exc} vs Exception Items (Prop 1e-5)\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for idx, exc in enumerate(exception_items):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Plot margin over time for each seed\n",
    "        for seed in range(seeds_n):\n",
    "            # Get margin for (non_exc, exc)\n",
    "            margin_data = extractor.training_progress[seed, :, non_exc, exc]\n",
    "            ax.plot(margin_data, alpha=0.5, linewidth=1)\n",
    "        \n",
    "        # Calculate mean and plot\n",
    "        mean_margin = extractor.training_progress[:, :, non_exc, exc].mean(axis=0)\n",
    "        ax.plot(mean_margin, color='black', linewidth=2, label='Mean', linestyle='--')\n",
    "        \n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Margin')\n",
    "        ax.set_title(f'({non_exc}, {exc})')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='red', linestyle=':', linewidth=1, alpha=0.5)\n",
    "        \n",
    "    axes[0].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot version - showing final margins for non-exception vs exception items\n",
    "# Reuse the same exception/non-exception item definitions\n",
    "\n",
    "# Get final timestep margins\n",
    "final_timestep = extractor.time_steps - 1\n",
    "\n",
    "for non_exc in non_exception_items:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    fig.suptitle(f\"Item {non_exc} vs Exception Items (Final Margins - Scatterplot)\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for idx, exc in enumerate(exception_items):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get final margins for all seeds for pair (non_exc, exc)\n",
    "        final_margins = extractor.training_progress[:, final_timestep, non_exc, exc]\n",
    "        \n",
    "        # Create scatter plot with seed indices on x-axis\n",
    "        seeds = np.arange(csv_seeds_n)\n",
    "        ax.scatter(seeds, final_margins, s=100, alpha=0.7, c='steelblue', edgecolors='black', linewidth=1)\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_margin = final_margins.mean()\n",
    "        ax.axhline(y=mean_margin, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_margin:.3f}')\n",
    "        \n",
    "        # Add zero reference line\n",
    "        ax.axhline(y=0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Seed')\n",
    "        ax.set_ylabel('Final Margin')\n",
    "        ax.set_title(f'({non_exc}, {exc})')\n",
    "        ax.set_xticks(seeds)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined scatter plot: all non-exception items vs exception items in one figure\n",
    "# X-axis: exception item index (2, 3, 4)\n",
    "# Y-axis: final margin values (one point per seed)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get final timestep margins\n",
    "final_timestep = extractor.time_steps - 1\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Create a 2x2 subplot grid for all non-exception items\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()  # Flatten to easily iterate\n",
    "\n",
    "fig.suptitle('Non-Exception Items vs Exception Items (All Seeds)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Create a plot for each non-exception item\n",
    "for idx, non_exc_item in enumerate(non_exception_items):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for exc in exception_items:\n",
    "        # Get final margins for all seeds for pair (non_exc_item, exc)\n",
    "        final_margins = extractor.training_progress[:, final_timestep, non_exc_item, exc]\n",
    "        \n",
    "        # Correct margins: multiply by -1 if i > j\n",
    "        if non_exc_item > exc:\n",
    "            final_margins = -1 * final_margins\n",
    "        \n",
    "        # Add jitter to x-coordinates for visibility\n",
    "        x = rng.normal(loc=exc, scale=0.06, size=final_margins.size)\n",
    "        ax.scatter(x, final_margins, alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        # Add mean line with label\n",
    "        mean_margin = final_margins.mean()\n",
    "        ax.hlines(mean_margin, exc - 0.25, exc + 0.25, colors='red', linestyles='--', linewidth=2)\n",
    "        ax.text(exc, mean_margin, f'{mean_margin:.2f}', ha='center', va='bottom', fontsize=8, \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='red', alpha=0.8))\n",
    "    \n",
    "    ax.set_xticks(exception_items)\n",
    "    ax.set_xticklabels([str(e) for e in exception_items])\n",
    "    ax.set_xlabel('Exception Item')\n",
    "    ax.set_ylabel('Final Margin')\n",
    "    ax.set_title(f'Corrected Margin on Item {non_exc_item} vs Exception Items', fontweight='bold')\n",
    "    ax.axhline(0, color='gray', linewidth=1, linestyle=':', alpha=0.5)\n",
    "    ax.set_xlim(min(exception_items) - 0.5, max(exception_items) + 0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined scatter plot with SIGMOID(corrected margins)\n",
    "# X-axis: exception item index (2, 3, 4)\n",
    "# Y-axis: sigmoid of final margin values (one point per seed)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get final timestep margins\n",
    "final_timestep = extractor.time_steps - 1\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Create a 2x2 subplot grid for all non-exception items\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()  # Flatten to easily iterate\n",
    "\n",
    "fig.suptitle('Non-Exception Items vs Exception Items - Sigmoid(Corrected Margin) (Prop=1e-5)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Create a plot for each non-exception item\n",
    "for idx, non_exc_item in enumerate(non_exception_items):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for exc in exception_items:\n",
    "        # Get final margins for all seeds for pair (non_exc_item, exc)\n",
    "        final_margins = extractor.training_progress[:, final_timestep, non_exc_item, exc]\n",
    "        \n",
    "        # Correct margins: multiply by -1 if i > j\n",
    "        if non_exc_item > exc:\n",
    "            final_margins = -1 * final_margins\n",
    "        \n",
    "        # Apply sigmoid transformation\n",
    "        final_margins_sigmoid = 1 / (1 + np.exp(-final_margins))\n",
    "        \n",
    "        # Add jitter to x-coordinates for visibility\n",
    "        x = rng.normal(loc=exc, scale=0.06, size=final_margins_sigmoid.size)\n",
    "        ax.scatter(x, final_margins_sigmoid, alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        # Add mean line with label\n",
    "        mean_margin = final_margins_sigmoid.mean()\n",
    "        ax.hlines(mean_margin, exc - 0.25, exc + 0.25, colors='red', linestyles='--', linewidth=2)\n",
    "        ax.text(exc, mean_margin, f'{mean_margin:.3f}', ha='center', va='bottom', fontsize=8, \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='red', alpha=0.8))\n",
    "    \n",
    "    ax.set_xticks(exception_items)\n",
    "    ax.set_xticklabels([str(e) for e in exception_items])\n",
    "    ax.set_xlabel('Exception Item')\n",
    "    ax.set_ylabel('Sigmoid(Corrected Margin)')\n",
    "    ax.set_title(f'Item {non_exc_item} vs Exception Items', fontweight='bold')\n",
    "    ax.axhline(0.5, color='gray', linewidth=1, linestyle=':', alpha=0.5, label='Decision boundary')\n",
    "    ax.set_xlim(min(exception_items) - 0.5, max(exception_items) + 0.5)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    if idx == 0:\n",
    "        ax.legend(loc='lower right', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Margins for exception training pair, adjacent training pairs, and generalization pairs (sd ≥ 2)\n",
    "\n",
    "# Build extractor from CSV to make this cell self-contained\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure we can import MarginExtractor\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "from csv_margin import MarginExtractor\n",
    "\n",
    "# Load CSV written earlier in the notebook\n",
    "csv_filename = \"conjunctive_lazy_rich.csv\"\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Reconstruct training_progress from the CSV (same logic as earlier)\n",
    "t_col, i_col, j_col = 'time_step', 'i', 'j'\n",
    "margin_cols = [col for col in df.columns if col not in [t_col, i_col, j_col, 'mean_margin', 'std_margin']]\n",
    "csv_seeds_n = len(margin_cols)\n",
    "csv_time_steps = df[t_col].max() + 1\n",
    "csv_items_n = max(df[i_col].max(), df[j_col].max()) + 1\n",
    "\n",
    "training_progress = np.zeros((csv_seeds_n, csv_time_steps, csv_items_n, csv_items_n))\n",
    "for _, row in df.iterrows():\n",
    "    t = int(row[t_col]); i = int(row[i_col]); j = int(row[j_col])\n",
    "    for s, col in enumerate(margin_cols):\n",
    "        training_progress[s, t, i, j] = row[col]\n",
    "\n",
    "results_from_csv = {\"train\": {\"training_progress\": training_progress}}\n",
    "extractor = MarginExtractor(results_from_csv)\n",
    "\n",
    "# Exception definition\n",
    "p, q = 4, 2\n",
    "\n",
    "# Ensure we have the final timestep and items count\n",
    "final_timestep = extractor.time_steps - 1\n",
    "items_n = extractor.items_n if hasattr(extractor, 'items_n') else extractor.training_progress.shape[2]\n",
    "\n",
    "# Use unordered pairs by taking only i < j and define categories by symbolic distance (sd = |i - j|)\n",
    "exc_unordered = (min(p, q), max(p, q))  # the single exception training pair (unordered)\n",
    "\n",
    "training_pairs = [(i, j) for i in range(items_n) for j in range(i + 1, items_n)\n",
    "                  if abs(i - j) == 1 and (i, j) != exc_unordered]\n",
    "\n",
    "exception_pairs = [exc_unordered]\n",
    "\n",
    "generalization_pairs = [(i, j) for i in range(items_n) for j in range(i + 1, items_n)\n",
    "                        if abs(i - j) >= 2]\n",
    "\n",
    "# Helper to get corrected margins across seeds for a single (i, j) with i < j\n",
    "# Flip the exception pair so it aligns with the \"corrected\" convention used elsewhere\n",
    "\n",
    "def corrected_seed_margins(i, j):\n",
    "    vals = extractor.training_progress[:, final_timestep, i, j]\n",
    "    if (i, j) == exc_unordered:\n",
    "        vals = -vals\n",
    "    return vals\n",
    "\n",
    "# Collect margins for a set of pairs (concatenate all seeds across all pairs)\n",
    "\n",
    "def collect_margins(pairs):\n",
    "    if len(pairs) == 0:\n",
    "        return np.array([])\n",
    "    all_vals = [corrected_seed_margins(i, j) for (i, j) in pairs]\n",
    "    return np.concatenate(all_vals, axis=0)\n",
    "\n",
    "exception_margins = collect_margins(exception_pairs)\n",
    "training_margins = collect_margins(training_pairs)\n",
    "generalization_margins = collect_margins(generalization_pairs)\n",
    "\n",
    "# Jittered scatter per category with mean lines\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "categories = [\"Exception\\ntraining pair\", \"Adjacent\\ntraining pairs\", \"Generalization\\n(sd ≥ 2)\"]\n",
    "values = [exception_margins, training_margins, generalization_margins]\n",
    "colors = ['crimson', 'steelblue', 'seagreen']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for idx, (vals, color) in enumerate(zip(values, colors)):\n",
    "    if vals.size == 0:\n",
    "        continue\n",
    "    x = rng.normal(loc=idx, scale=0.06, size=vals.size)\n",
    "    plt.scatter(x, vals, alpha=0.6, s=30, edgecolors='black', linewidth=0.5, color=color)\n",
    "    mean_val = vals.mean()\n",
    "    plt.hlines(mean_val, idx - 0.25, idx + 0.25, colors='red', linestyles='--', linewidth=2)\n",
    "    plt.text(idx, mean_val, f\"{mean_val:.2f}\", ha='center', va='bottom', fontsize=9,\n",
    "             bbox=dict(boxstyle='round,pad=0.25', facecolor='white', edgecolor='red', alpha=0.8))\n",
    "\n",
    "plt.xticks(range(len(categories)), [f\"{label}\\n(n={len(v)})\" for label, v in zip(categories, values)])\n",
    "plt.ylabel('Corrected margin (final timestep)')\n",
    "plt.title('Margins: exception training, adjacent training, and generalization (sd ≥ 2)')\n",
    "plt.axhline(0, color='gray', linewidth=1, linestyle=':', alpha=0.5)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average category margins over time: exception training, adjacent training, generalization (sd ≥ 2)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume extractor, p, q exist from previous cell\n",
    "items_n = extractor.items_n if hasattr(extractor, 'items_n') else extractor.training_progress.shape[2]\n",
    "time_steps = extractor.time_steps\n",
    "seeds_n = extractor.seeds_n\n",
    "\n",
    "# Define unordered exception pair and category sets using i < j canonical orientation\n",
    "exc_unordered = (min(p, q), max(p, q))\n",
    "training_pairs = [(i, j) for i in range(items_n) for j in range(i + 1, items_n)\n",
    "                  if abs(i - j) == 1 and (i, j) != exc_unordered]\n",
    "exception_pairs = [exc_unordered]\n",
    "generalization_pairs = [(i, j) for i in range(items_n) for j in range(i + 1, items_n)\n",
    "                        if abs(i - j) >= 2]\n",
    "\n",
    "# Helpers to gather corrected margins at time t for a category\n",
    "\n",
    "def corrected_margins_at_t(i, j, t):\n",
    "    vals = extractor.training_progress[:, t, i, j]\n",
    "    # Flip the exception pair so it aligns with the \"corrected\" convention\n",
    "    if (i, j) == exc_unordered:\n",
    "        vals = -vals\n",
    "    return vals\n",
    "\n",
    "\n",
    "def category_mean_series(pairs):\n",
    "    means = np.zeros(time_steps)\n",
    "    if len(pairs) == 0:\n",
    "        return means\n",
    "    for t in range(time_steps):\n",
    "        all_vals_t = []\n",
    "        for (i, j) in pairs:\n",
    "            all_vals_t.append(corrected_margins_at_t(i, j, t))  # shape: (seeds,)\n",
    "        cat_vals_t = np.concatenate(all_vals_t, axis=0)  # concat over pairs and seeds\n",
    "        means[t] = cat_vals_t.mean()\n",
    "    return means\n",
    "\n",
    "exc_series = category_mean_series(exception_pairs)\n",
    "train_series = category_mean_series(training_pairs)\n",
    "gen_series = category_mean_series(generalization_pairs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(exc_series, label='Exception training pair', color='crimson', linewidth=2)\n",
    "plt.plot(train_series, label='Adjacent training pairs (mean)', color='steelblue', linewidth=2)\n",
    "plt.plot(gen_series, label='Generalization (sd ≥ 2) (mean)', color='seagreen', linewidth=2)\n",
    "plt.axhline(0, color='gray', linestyle=':', linewidth=1, alpha=0.6)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Corrected margin')\n",
    "plt.title('Average margins over time by category')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05cc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average category margins over time (4 categories):\n",
    "# 1) Exception training pair\n",
    "# 2) Adjacent exception training pairs (within exception band)\n",
    "# 3) Adjacent non-exception training pairs (outside exception band)\n",
    "# 4) Generalization pairs (sd ≥ 2)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume extractor, p, q exist from previous cell\n",
    "items_n = extractor.items_n if hasattr(extractor, 'items_n') else extractor.training_progress.shape[2]\n",
    "time_steps = extractor.time_steps\n",
    "\n",
    "exc_unordered = (min(p, q), max(p, q))\n",
    "exception_items = list(range(min(p, q), max(p, q) + 1))\n",
    "\n",
    "# Build categories using canonical orientation i < j\n",
    "exception_pair = [exc_unordered]\n",
    "\n",
    "adjacent_exception_pairs = [\n",
    "    (i, j)\n",
    "    for i in range(items_n)\n",
    "    for j in range(i + 1, items_n)\n",
    "    if abs(i - j) == 1 and (i in exception_items and j in exception_items)\n",
    "]\n",
    "\n",
    "adjacent_nonexception_pairs = [\n",
    "    (i, j)\n",
    "    for i in range(items_n)\n",
    "    for j in range(i + 1, items_n)\n",
    "    if abs(i - j) == 1 and not (i in exception_items and j in exception_items)\n",
    "]\n",
    "\n",
    "generalization_pairs = [\n",
    "    (i, j)\n",
    "    for i in range(items_n)\n",
    "    for j in range(i + 1, items_n)\n",
    "    if abs(i - j) >= 2\n",
    "]\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def corrected_margins_at_t(i, j, t):\n",
    "    vals = extractor.training_progress[:, t, i, j]\n",
    "    # Flip the exception pair to align with the \"corrected\" convention\n",
    "    if (i, j) == exc_unordered:\n",
    "        vals = -vals\n",
    "    return vals\n",
    "\n",
    "\n",
    "def category_mean_series(pairs):\n",
    "    means = np.zeros(time_steps)\n",
    "    if len(pairs) == 0:\n",
    "        return means\n",
    "    for t in range(time_steps):\n",
    "        all_vals_t = [corrected_margins_at_t(i, j, t) for (i, j) in pairs]\n",
    "        cat_vals_t = np.concatenate(all_vals_t, axis=0)\n",
    "        means[t] = cat_vals_t.mean()\n",
    "    return means\n",
    "\n",
    "exc_series = category_mean_series(exception_pair)\n",
    "adj_exc_series = category_mean_series(adjacent_exception_pairs)\n",
    "adj_nonexc_series = category_mean_series(adjacent_nonexception_pairs)\n",
    "gen_series = category_mean_series(generalization_pairs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(exc_series, label='Exception training pair', color='crimson', linewidth=2)\n",
    "plt.plot(adj_exc_series, label='Adjacent exception training pairs', color='orange', linewidth=2)\n",
    "plt.plot(adj_nonexc_series, label='Adjacent non-exception training pairs', color='steelblue', linewidth=2)\n",
    "plt.plot(gen_series, label='Generalization (sd ≥ 2)', color='seagreen', linewidth=2)\n",
    "plt.axhline(0, color='gray', linestyle=':', linewidth=1, alpha=0.6)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Corrected margin')\n",
    "plt.title('Average margins over time by category (4-way split)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22efbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"adjacent_nonexception_pairs:\", adjacent_nonexception_pairs)\n",
    "print(\"adjacent_exception_pairs:\", adjacent_exception_pairs)\n",
    "print(\"exception_pair:\", exception_pair)\n",
    "print(\"generalization_pairs:\", generalization_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f93d6",
   "metadata": {},
   "source": [
    "# Load CSVs for Respresentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfaacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_layer_from_csv_gz(csv_gz_path):\n",
    "    df = pd.read_csv(csv_gz_path, compression='gzip')\n",
    "    seed_cols = [c for c in df.columns if c.startswith('seed_')]\n",
    "    time_steps = int(df['time_step'].max()) + 1\n",
    "    items_n_inferred = int(df['item'].max()) + 1\n",
    "    units = int(df['unit'].max()) + 1\n",
    "    seeds_n = len(seed_cols)\n",
    "    arr = np.zeros((seeds_n, time_steps, items_n_inferred, units))\n",
    "    for _, row in df.iterrows():\n",
    "        t = int(row['time_step']); i = int(row['item']); u = int(row['unit'])\n",
    "        for s_idx, sc in enumerate(seed_cols):\n",
    "            arr[s_idx, t, i, u] = row[sc]\n",
    "    return arr  # shape: (seeds, time, items, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1370dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Knowledge Assembly-style visualization adapted to this network\n",
    "# - Top row: Euclidean distance matrices of hidden layer h1s at 3 training positions\n",
    "# - Bottom row: 2D MDS embeddings at the same positions\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import pandas as pd\n",
    "from helpers import rotate\n",
    "from plotting import mds_plot\n",
    "\n",
    "# Local CSV.GZ loader for h1s/h2s saved earlier in this notebook\n",
    "def load_layer_from_csv_gz(csv_gz_path):\n",
    "    df = pd.read_csv(csv_gz_path, compression='gzip')\n",
    "    seed_cols = [c for c in df.columns if c.startswith('seed_')]\n",
    "    time_steps = int(df['time_step'].max()) + 1\n",
    "    items_n = int(df['item'].max()) + 1\n",
    "    units = int(df['unit'].max()) + 1\n",
    "    seeds_n = len(seed_cols)\n",
    "    arr = np.zeros((seeds_n, time_steps, items_n, units))\n",
    "    for _, row in df.iterrows():\n",
    "        t = int(row['time_step']); i = int(row['item']); u = int(row['unit'])\n",
    "        for s_idx, sc in enumerate(seed_cols):\n",
    "            arr[s_idx, t, i, u] = row[sc]\n",
    "    return arr\n",
    "\n",
    "# Local safe matrix plot that works for odd items_n (do not rely on plotting.matrix_plot)\n",
    "def matrix_plot_safe(data, ax, items_n, vmin=None, vmax=None):\n",
    "    if vmin is None:\n",
    "        im = ax.imshow(data, cmap=plt.cm.magma)\n",
    "    else:\n",
    "        im = ax.imshow(data, cmap=plt.cm.magma, vmin=vmin, vmax=vmax)\n",
    "    ax.set_xticks(range(items_n))\n",
    "    ax.set_yticks(range(items_n))\n",
    "    if items_n % 2 == 0:\n",
    "        labels = list(range(1, items_n // 2 + 1)) * 2\n",
    "    else:\n",
    "        labels = list(range(1, items_n + 1))\n",
    "    ax.set_xticklabels(labels, va='center', fontsize=10, fontweight=\"bold\")\n",
    "    ax.set_yticklabels(labels, ha='center', fontsize=10, fontweight=\"bold\")\n",
    "    ax.tick_params(left=False, bottom=False)\n",
    "    for pos in [\"top\", \"right\", \"bottom\", \"left\"]:\n",
    "        ax.spines[pos].set_linewidth(1)\n",
    "        ax.spines[pos].set_color([0.5] * 3)\n",
    "    return im\n",
    "\n",
    "# Load h1s from gzipped CSV written by the CSV export cell\n",
    "csv_gz_h1_path = r\"C:\\Users\\lukel\\iCloudDrive\\Computer\\Current\\0. Columbia\\Research\\Nelli Reimplementation\\conjunctive_lazy_rich_1e-5\\conjunctive_lazy_rich_h1s.csv.gz\"\n",
    "h1s_loaded = load_layer_from_csv_gz(csv_gz_h1_path)\n",
    "all_h1s = h1s_loaded.mean(axis=0)  # (time_steps, items_n, h1_size)\n",
    "items_n = all_h1s.shape[1]\n",
    "steps_after_training = all_h1s.shape[0] - 1\n",
    "mds_seed = 0\n",
    "\n",
    "# Load h2s from gzipped CSV written by the CSV export cell\n",
    "csv_gz_h2_path = r\"C:\\Users\\lukel\\iCloudDrive\\Computer\\Current\\0. Columbia\\Research\\Nelli Reimplementation\\conjunctive_lazy_rich_1e-5\\conjunctive_lazy_rich_h2s.csv.gz\"\n",
    "h2s_loaded = load_layer_from_csv_gz(csv_gz_h2_path)\n",
    "all_h2s = h2s_loaded.mean(axis=0)  # (time_steps, items_n, h2_size)\n",
    "items_n = all_h2s.shape[1]\n",
    "steps_after_training = all_h2s.shape[0] - 1\n",
    "mds_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad3641",
   "metadata": {},
   "source": [
    "# Plot H1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86012617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose nine positions across training\n",
    "positions = [0.00, 0.10, 0.20, 0.50, 0.65, 0.75, 0.85, 0.90, 1.00]\n",
    "\n",
    "fig1, axs1 = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "fig1.patch.set_facecolor('white')\n",
    "\n",
    "fig2, axs2 = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "fig2.patch.set_facecolor('white')\n",
    "\n",
    "# Determine vmax across selected time slices\n",
    "vmax = 0.0\n",
    "for i in range(len(positions)):\n",
    "    t_idx = max(0, int(positions[i] * steps_after_training))\n",
    "    h1s = all_h1s[t_idx]\n",
    "    vmax_ = float(np.round(np.max(euclidean_distances(h1s)), 1))\n",
    "    if vmax_ > vmax:\n",
    "        vmax = vmax_\n",
    "\n",
    "# Distance matrices → 3x3 grid (fig1)\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    t_idx = max(0, int(p * steps_after_training))\n",
    "    dists = euclidean_distances(all_h1s[t_idx])\n",
    "    im = matrix_plot_safe(dists, axs1[r, c], items_n, 0., vmax)\n",
    "    axs1[r, c].set_title(f\"step {t_idx+1}\")\n",
    "\n",
    "cbar = fig1.colorbar(im, ax=axs1, shrink=.6, aspect=25)\n",
    "cbar.set_ticks([])\n",
    "cbar.ax.set_ylabel(\"a.u.\")\n",
    "\n",
    "# MDS embeddings → 3x3 grid (fig2)\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    t_idx = max(0, int(p * steps_after_training))\n",
    "    dists = euclidean_distances(all_h1s[t_idx])\n",
    "    mds = manifold.MDS(\n",
    "        n_components=2,\n",
    "        eps=1e-9,\n",
    "        dissimilarity=\"precomputed\",\n",
    "        random_state=mds_seed,\n",
    "        n_init=4,                     # keep behavior stable; silences FutureWarning\n",
    "        normalized_stress=\"auto\",\n",
    "    )\n",
    "    d1, d2 = mds.fit(dists).embedding_.T\n",
    "\n",
    "    if idx % 3 == 0:\n",
    "        rotate(d1, d2, 250.)\n",
    "    elif idx % 3 == 1:\n",
    "        rotate(d1, d2, 230.)\n",
    "    else:\n",
    "        rotate(d1, d2, 260.)\n",
    "\n",
    "    mds_plot((d1, d2), axs2[r, c], items_n)\n",
    "    axs2[r, c].set_title(f\"step {t_idx+1}\")\n",
    "\n",
    "    for pos in [\"left\", \"bottom\"]:\n",
    "        axs2[r, c].spines[pos].set_linewidth(1)\n",
    "        axs2[r, c].spines[pos].set_color([0.5]*3)\n",
    "\n",
    "    axs2[r, c].set_xlabel(\"dimension 1\\na.u.\")\n",
    "    axs2[r, c].set_ylabel(\"dimension 2\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Kruskal stress vs dimensions for the same positions (h1)\n",
    "max_dims = 8\n",
    "fig_stress, axs_stress = plt.subplots(3, 3, figsize=(12., 10.), constrained_layout=True)\n",
    "fig_stress.patch.set_facecolor('white')\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    t_idx = max(0, int(p * steps_after_training))\n",
    "    D = euclidean_distances(all_h1s[t_idx])\n",
    "    denom = np.sum(D**2)\n",
    "    stresses = []\n",
    "    for k in range(1, max_dims + 1):\n",
    "        m = manifold.MDS(n_components=k, dissimilarity=\"precomputed\", random_state=mds_seed, n_init=4, normalized_stress=\"auto\")\n",
    "        m.fit(D)\n",
    "        stress_raw = getattr(m, \"stress_\", None)\n",
    "        if stress_raw is None or denom == 0:\n",
    "            stresses.append(np.nan)\n",
    "        else:\n",
    "            stresses.append(np.sqrt(stress_raw / denom))  # Kruskal stress-1\n",
    "    axs_stress[r, c].plot(range(1, max_dims + 1), stresses, marker='o')\n",
    "    axs_stress[r, c].set_title(f\"step {t_idx+1}\")\n",
    "    if r == 2:\n",
    "        axs_stress[r, c].set_xlabel(\"dimensions\")\n",
    "    if c == 0:\n",
    "        axs_stress[r, c].set_ylabel(\"Kruskal stress-1\")\n",
    "    axs_stress[r, c].set_ylim(0.0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cc7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot H2\n",
    "\n",
    "# Choose nine positions across training\n",
    "positions = [0.00, 0.10, 0.20, 0.50, 0.65, 0.75, 0.85, 0.90, 1.00]\n",
    "\n",
    "fig1, axs1 = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "fig1.patch.set_facecolor('white')\n",
    "\n",
    "fig2, axs2 = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "fig2.patch.set_facecolor('white')\n",
    "\n",
    "# Determine vmax across selected time slices\n",
    "vmax = 0.0\n",
    "for i in range(len(positions)):\n",
    "    t_idx = max(0, int(positions[i] * steps_after_training))\n",
    "    h2s = all_h2s[t_idx]\n",
    "    vmax_ = float(np.round(np.max(euclidean_distances(h2s)), 1))\n",
    "    if vmax_ > vmax:\n",
    "        vmax = vmax_\n",
    "\n",
    "# Distance matrices → 3x3 grid (fig1)\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    t_idx = max(0, int(p * steps_after_training))\n",
    "    dists = euclidean_distances(all_h2s[t_idx])\n",
    "    im = matrix_plot_safe(dists, axs1[r, c], items_n, 0., vmax)\n",
    "    axs1[r, c].set_title(f\"step {t_idx+1}\")\n",
    "\n",
    "cbar = fig1.colorbar(im, ax=axs1, shrink=.6, aspect=25)\n",
    "cbar.set_ticks([])\n",
    "cbar.ax.set_ylabel(\"a.u.\")\n",
    "\n",
    "# MDS embeddings → 3x3 grid (fig2)\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    t_idx = max(0, int(p * steps_after_training))\n",
    "    dists = euclidean_distances(all_h2s[t_idx])\n",
    "    mds = manifold.MDS(\n",
    "        n_components=2,\n",
    "        eps=1e-9,\n",
    "        dissimilarity=\"precomputed\",\n",
    "        random_state=mds_seed,\n",
    "        n_init=4,                     # keep behavior stable; silences FutureWarning\n",
    "        normalized_stress=\"auto\",\n",
    "    )\n",
    "    d1, d2 = mds.fit(dists).embedding_.T\n",
    "\n",
    "    if idx % 3 == 0:\n",
    "        rotate(d1, d2, 250.)\n",
    "    elif idx % 3 == 1:\n",
    "        rotate(d1, d2, 230.)\n",
    "    else:\n",
    "        rotate(d1, d2, 260.)\n",
    "\n",
    "    mds_plot((d1, d2), axs2[r, c], items_n)\n",
    "    axs2[r, c].set_title(f\"step {t_idx+1}\")\n",
    "\n",
    "    for pos in [\"left\", \"bottom\"]:\n",
    "        axs2[r, c].spines[pos].set_linewidth(1)\n",
    "        axs2[r, c].spines[pos].set_color([0.5]*3)\n",
    "\n",
    "    axs2[r, c].set_xlabel(\"dimension 1\\na.u.\")\n",
    "    axs2[r, c].set_ylabel(\"dimension 2\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Kruskal stress vs dimensions for the same positions (h2)\n",
    "max_dims = 8\n",
    "fig_stress_h2, axs_stress_h2 = plt.subplots(3, 3, figsize=(12., 10.), constrained_layout=True)\n",
    "fig_stress_h2.patch.set_facecolor('white')\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    t_idx = max(0, int(p * steps_after_training))\n",
    "    D = euclidean_distances(all_h2s[t_idx])\n",
    "    denom = np.sum(D**2)\n",
    "    stresses = []\n",
    "    for k in range(1, max_dims + 1):\n",
    "        m = manifold.MDS(n_components=k, dissimilarity=\"precomputed\", random_state=mds_seed, n_init=4, normalized_stress=\"auto\")\n",
    "        m.fit(D)\n",
    "        stress_raw = getattr(m, \"stress_\", None)\n",
    "        if stress_raw is None or denom == 0:\n",
    "            stresses.append(np.nan)\n",
    "        else:\n",
    "            stresses.append(np.sqrt(stress_raw / denom))  # Kruskal stress-1\n",
    "    axs_stress_h2[r, c].plot(range(1, max_dims + 1), stresses, marker='o')\n",
    "    axs_stress_h2[r, c].set_title(f\"step {t_idx+1}\")\n",
    "    if r == 2:\n",
    "        axs_stress_h2[r, c].set_xlabel(\"dimensions\")\n",
    "    if c == 0:\n",
    "        axs_stress_h2[r, c].set_ylabel(\"Kruskal stress-1\")\n",
    "    axs_stress_h2[r, c].set_ylim(0.0, 1.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24757f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representational Similarity Matrices (RSM) for h2 with normalization (Pearson correlation)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unwrap results and check availability\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "train_dict = results.get(\"train\", {}) if isinstance(results, dict) else {}\n",
    "if \"h2s\" not in train_dict or train_dict[\"h2s\"] is None:\n",
    "    print(\"No h2s found in results['train'].\")\n",
    "else:\n",
    "    # h2s shape: (seeds_n, time_steps, items_n, h2_size)\n",
    "    H2 = train_dict[\"h2s\"]\n",
    "    H2_avg = np.nan_to_num(H2.mean(axis=0), nan=0.0, posinf=0.0, neginf=0.0)  # (time_steps, items_n, h2_size)\n",
    "    time_steps, items_n_inferred, h2_size = H2_avg.shape\n",
    "\n",
    "    # Positions to display\n",
    "    positions = [0.00, 0.10, 0.20, 0.50, 0.65, 0.75, 0.85, 0.90, 1.00]\n",
    "    steps_after_training = time_steps - 1\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    im = None\n",
    "    for idx, p in enumerate(positions):\n",
    "        r, c = divmod(idx, 3)\n",
    "        t_idx = max(0, int(p * steps_after_training))\n",
    "        X = H2_avg[t_idx]  # (items_n, h2_size)\n",
    "        # Normalize each item's vector (z-score across features) for Pearson correlation\n",
    "        X = X - X.mean(axis=1, keepdims=True)\n",
    "        denom = X.std(axis=1, keepdims=True) + 1e-8\n",
    "        Xn = X / denom\n",
    "        # Pearson similarity between items (i by j)\n",
    "        sim = (Xn @ Xn.T) / Xn.shape[1]\n",
    "        sim = np.clip(sim, -1.0, 1.0)\n",
    "        # Handle any numerical NaNs\n",
    "        sim = np.nan_to_num(sim, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        ax = axs[r, c]\n",
    "        im = ax.imshow(sim, vmin=-1, vmax=1, cmap='coolwarm')\n",
    "        ax.set_title(f\"RSM h2 — step {t_idx+1}\")\n",
    "        ax.set_xticks(range(items_n_inferred))\n",
    "        ax.set_yticks(range(items_n_inferred))\n",
    "        ax.set_xticklabels([str(i) for i in range(items_n_inferred)])\n",
    "        ax.set_yticklabels([str(i) for i in range(items_n_inferred)])\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "        for pos in [\"top\", \"right\", \"bottom\", \"left\"]:\n",
    "            ax.spines[pos].set_linewidth(1)\n",
    "            ax.spines[pos].set_color([0.5] * 3)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=.6, aspect=25)\n",
    "    cbar.set_label('Pearson r')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d797510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDS of all ordered pairs (h1, from CSV) at all requested steps, with Kruskal stress-1 and (raw) Kruskal stress\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "# Use CSV-loaded h1 data if available; else load from gz in this folder\n",
    "if 'all_h1s' in globals():\n",
    "    H1_avg = all_h1s  # shape: (time_steps, items_n, h1_size) already averaged across seeds\n",
    "else:\n",
    "    def load_h1_mean_from_csv_gz(csv_gz_path):\n",
    "        df = pd.read_csv(csv_gz_path, compression='gzip')\n",
    "        seed_cols = [c for c in df.columns if c.startswith('seed_')]\n",
    "        # Mean across seeds per (time_step, item, unit)\n",
    "        grouped = df.groupby(['time_step', 'item', 'unit'])[seed_cols].mean()\n",
    "        mean_series = grouped.mean(axis=1)  # average across seed columns\n",
    "        time_steps = int(df['time_step'].max()) + 1\n",
    "        items_n_inferred = int(df['item'].max()) + 1\n",
    "        units = int(df['unit'].max()) + 1\n",
    "        arr = np.zeros((time_steps, items_n_inferred, units))\n",
    "        for (t, i, u), val in mean_series.items():\n",
    "            arr[int(t), int(i), int(u)] = float(val)\n",
    "        return arr\n",
    "    # Try local gz produced earlier in this notebook\n",
    "    H1_avg = load_h1_mean_from_csv_gz('conjunctive_lazy_rich_h1s.csv.gz')\n",
    "\n",
    "time_steps, items_n_inferred, h1_size = H1_avg.shape\n",
    "\n",
    "# Positions to display, used previously\n",
    "positions = [0.00, 0.10, 0.20, 0.50, 0.65, 0.75, 0.85, 0.90, 1.00]\n",
    "steps_after_training = time_steps - 1\n",
    "step_idxs = [max(0, int(p * steps_after_training)) for p in positions]\n",
    "\n",
    "# Utility to build all concatenated ordered pairs (i!=j)\n",
    "def build_pairs(X_items):\n",
    "    vecs, labels, pairs = [], [], []\n",
    "    for i in range(items_n_inferred):\n",
    "        for j in range(items_n_inferred):\n",
    "            if i == j:\n",
    "                continue\n",
    "            v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "            vecs.append(v)\n",
    "            labels.append(f\"({i},{j})\")\n",
    "            pairs.append((i, j))\n",
    "    V = np.asarray(vecs)\n",
    "    # mean-center features in each matrix for each step, stabilizes MDS\n",
    "    V = V - V.mean(axis=0, keepdims=True)\n",
    "    return V, labels, pairs\n",
    "\n",
    "# Fit 2D metric MDS on each distance matrix; report Kruskal stress-1 and Kruskal stress (raw)\n",
    "def fit_mds_with_stress(D, seed):\n",
    "    mds = manifold.MDS(\n",
    "        n_components=2,\n",
    "        dissimilarity='precomputed',\n",
    "        random_state=seed,\n",
    "        n_init=4,\n",
    "        normalized_stress='auto',\n",
    "    )\n",
    "    fit_result = mds.fit(D)\n",
    "    coords = fit_result.embedding_\n",
    "    stress_raw = getattr(mds, 'stress_', None)\n",
    "    denom = float(np.sum(D ** 2))\n",
    "    # Kruskal's stress-1 as in sklearn docs: sqrt( sum((d_ij - d̂_ij)^2) / sum(d_ij^2) )\n",
    "    stress1 = np.sqrt(stress_raw / denom) if (stress_raw is not None and denom > 0) else np.nan\n",
    "\n",
    "    # Compute the reconstructed distances\n",
    "    Dr = euclidean_distances(coords)\n",
    "    # Use only upper triangle (excluding diagonal)\n",
    "    orig = D[np.triu_indices_from(D, k=1)]\n",
    "    rec = Dr[np.triu_indices_from(Dr, k=1)]\n",
    "    # Kruskal's raw stress\n",
    "    kruskal_stress = np.sqrt(np.sum((orig - rec) ** 2) / np.sum(orig ** 2)) if np.sum(orig ** 2) > 0 else np.nan\n",
    "\n",
    "    # For reporting: also include Pearson r between dist matrices\n",
    "    if len(orig) > 1:\n",
    "        r, _ = pearsonr(orig, rec)\n",
    "    else:\n",
    "        r = np.nan\n",
    "\n",
    "    return coords, stress1, kruskal_stress, r\n",
    "\n",
    "seed = mds_seed if 'mds_seed' in globals() else 0\n",
    "\n",
    "nsteps = len(step_idxs)\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(nsteps / ncols))\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(5.5*ncols, 5*nrows), constrained_layout=True)\n",
    "\n",
    "for pidx, (pos, step_idx) in enumerate(zip(positions, step_idxs)):\n",
    "    V, labels, pairs = build_pairs(H1_avg[step_idx])\n",
    "    D = euclidean_distances(V)\n",
    "    coords, stress1, kst, r = fit_mds_with_stress(D, seed)\n",
    "    r_ax = pidx // ncols\n",
    "    c_ax = pidx % ncols\n",
    "    ax = axs[r_ax, c_ax] if nrows > 1 else axs[c_ax]\n",
    "    ax.scatter(coords[:, 0], coords[:, 1], s=18, color='k')\n",
    "    for i, lab in enumerate(labels):\n",
    "        ax.text(coords[i, 0], coords[i, 1], lab, fontsize=8, ha='left', va='center')\n",
    "    ax.axhline(0, color='lightgray', linewidth=1)\n",
    "    ax.axvline(0, color='lightgray', linewidth=1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    ax.set_title(\n",
    "        f\"step {step_idx+1} (pos={pos:.2f})\\n\"\n",
    "        f\"(stress-1={stress1:.3f}, kstress={kst:.3f}, r={r:.3f})\"\n",
    "    )\n",
    "    ax.set_xlabel('dimension 1')\n",
    "    ax.set_ylabel('dimension 2')\n",
    "\n",
    "# Remove any excess unused subplots\n",
    "for k in range(nsteps, nrows*ncols):\n",
    "    fig.delaxes(axs.flatten()[k])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDS of all ordered pairs (h2, from CSV) at all requested steps, with Kruskal stress-1 and (raw) Kruskal stress\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "# Use CSV-loaded h2 data if available; else load from gz in this folder\n",
    "if 'all_h2s' in globals():\n",
    "    H2_avg = all_h2s  # shape: (time_steps, items_n, h2_size) already averaged across seeds\n",
    "else:\n",
    "    def load_h2_mean_from_csv_gz(csv_gz_path):\n",
    "        df = pd.read_csv(csv_gz_path, compression='gzip')\n",
    "        seed_cols = [c for c in df.columns if c.startswith('seed_')]\n",
    "        # Mean across seeds per (time_step, item, unit)\n",
    "        grouped = df.groupby(['time_step', 'item', 'unit'])[seed_cols].mean()\n",
    "        mean_series = grouped.mean(axis=1)  # average across seed columns\n",
    "        time_steps = int(df['time_step'].max()) + 1\n",
    "        items_n_inferred = int(df['item'].max()) + 1\n",
    "        units = int(df['unit'].max()) + 1\n",
    "        arr = np.zeros((time_steps, items_n_inferred, units))\n",
    "        for (t, i, u), val in mean_series.items():\n",
    "            arr[int(t), int(i), int(u)] = float(val)\n",
    "        return arr\n",
    "    # Try local gz produced earlier in this notebook\n",
    "    H2_avg = load_h2_mean_from_csv_gz('conjunctive_lazy_rich_h2s.csv.gz')\n",
    "\n",
    "time_steps, items_n_inferred, h2_size = H2_avg.shape\n",
    "\n",
    "# Positions to display, used previously\n",
    "positions = [0.00, 0.10, 0.20, 0.50, 0.65, 0.75, 0.85, 0.90, 1.00]\n",
    "steps_after_training = time_steps - 1\n",
    "step_idxs = [max(0, int(p * steps_after_training)) for p in positions]\n",
    "\n",
    "# Utility to build all concatenated ordered pairs (i!=j)\n",
    "def build_pairs(X_items):\n",
    "    vecs, labels, pairs = [], [], []\n",
    "    for i in range(items_n_inferred):\n",
    "        for j in range(items_n_inferred):\n",
    "            if i == j:\n",
    "                continue\n",
    "            v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "            vecs.append(v)\n",
    "            labels.append(f\"({i},{j})\")\n",
    "            pairs.append((i, j))\n",
    "    V = np.asarray(vecs)\n",
    "    # mean-center features in each matrix for each step, stabilizes MDS\n",
    "    V = V - V.mean(axis=0, keepdims=True)\n",
    "    return V, labels, pairs\n",
    "\n",
    "# Fit 2D metric MDS on each distance matrix; report Kruskal stress-1 and Kruskal stress (raw)\n",
    "def fit_mds_with_stress(D, seed):\n",
    "    mds = manifold.MDS(\n",
    "        n_components=2,\n",
    "        dissimilarity='precomputed',\n",
    "        random_state=seed,\n",
    "        n_init=4,\n",
    "        normalized_stress='auto',\n",
    "    )\n",
    "    fit_result = mds.fit(D)\n",
    "    coords = fit_result.embedding_\n",
    "    stress_raw = getattr(mds, 'stress_', None)\n",
    "    denom = float(np.sum(D ** 2))\n",
    "    # Kruskal's stress-1 as in sklearn docs: sqrt( sum((d_ij - d̂_ij)^2) / sum(d_ij^2) )\n",
    "    stress1 = np.sqrt(stress_raw / denom) if (stress_raw is not None and denom > 0) else np.nan\n",
    "\n",
    "    # Compute the reconstructed distances\n",
    "    Dr = euclidean_distances(coords)\n",
    "    # Use only upper triangle (excluding diagonal)\n",
    "    orig = D[np.triu_indices_from(D, k=1)]\n",
    "    rec = Dr[np.triu_indices_from(Dr, k=1)]\n",
    "    # Kruskal's raw stress\n",
    "    kruskal_stress = np.sqrt(np.sum((orig - rec) ** 2) / np.sum(orig ** 2)) if np.sum(orig ** 2) > 0 else np.nan\n",
    "\n",
    "    # For reporting: also include Pearson r between dist matrices\n",
    "    if len(orig) > 1:\n",
    "        r, _ = pearsonr(orig, rec)\n",
    "    else:\n",
    "        r = np.nan\n",
    "\n",
    "    return coords, stress1, kruskal_stress, r\n",
    "\n",
    "seed = mds_seed if 'mds_seed' in globals() else 0\n",
    "\n",
    "nsteps = len(step_idxs)\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(nsteps / ncols))\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(5.5*ncols, 5*nrows), constrained_layout=True)\n",
    "\n",
    "for pidx, (pos, step_idx) in enumerate(zip(positions, step_idxs)):\n",
    "    V, labels, pairs = build_pairs(H2_avg[step_idx])\n",
    "    D = euclidean_distances(V)\n",
    "    coords, stress1, kst, r = fit_mds_with_stress(D, seed)\n",
    "    r_ax = pidx // ncols\n",
    "    c_ax = pidx % ncols\n",
    "    ax = axs[r_ax, c_ax] if nrows > 1 else axs[c_ax]\n",
    "    ax.scatter(coords[:, 0], coords[:, 1], s=18, color='k')\n",
    "    for i, lab in enumerate(labels):\n",
    "        ax.text(coords[i, 0], coords[i, 1], lab, fontsize=8, ha='left', va='center')\n",
    "    ax.axhline(0, color='lightgray', linewidth=1)\n",
    "    ax.axvline(0, color='lightgray', linewidth=1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    ax.set_title(\n",
    "        f\"step {step_idx+1} (pos={pos:.2f})\\n\"\n",
    "        f\"(stress-1={stress1:.3f}, kstress={kst:.3f}, r={r:.3f})\"\n",
    "    )\n",
    "    ax.set_xlabel('dimension 1')\n",
    "    ax.set_ylabel('dimension 2')\n",
    "\n",
    "# Remove any excess unused subplots\n",
    "for k in range(nsteps, nrows*ncols):\n",
    "    fig.delaxes(axs.flatten()[k])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the most recent MDS step, compute and display the distance matrices in the actual space and the embedding (MDS) space,\n",
    "# and also plot the difference between the two.\n",
    "\n",
    "# We'll use the latest position in the previous loop, i.e., last pos and last step_idx\n",
    "step_idx = step_idxs[-1]\n",
    "V, labels, pairs = build_pairs(H2_avg[step_idx])\n",
    "D_actual = euclidean_distances(V)\n",
    "\n",
    "# Fit MDS to get embedding\n",
    "coords, stress1, kst, r = fit_mds_with_stress(D_actual, seed)\n",
    "D_embedded = euclidean_distances(coords)\n",
    "\n",
    "# Compute the difference matrix (actual - embedded)\n",
    "D_diff = D_actual - D_embedded\n",
    "\n",
    "# Show as heatmaps side by side for visual comparison, with a third panel for the difference\n",
    "fig, axs = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# Actual distances\n",
    "im0 = axs[0].imshow(D_actual, cmap=\"viridis\")\n",
    "axs[0].set_title(\"Euclidean distances (actual pair space)\")\n",
    "axs[0].set_xlabel(\"Pair idx\")\n",
    "axs[0].set_ylabel(\"Pair idx\")\n",
    "plt.colorbar(im0, ax=axs[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Embedded distances\n",
    "im1 = axs[1].imshow(D_embedded, cmap=\"viridis\")\n",
    "axs[1].set_title(\"Euclidean distances (MDS embedding)\")\n",
    "axs[1].set_xlabel(\"Pair idx\")\n",
    "axs[1].set_ylabel(\"Pair idx\")\n",
    "plt.colorbar(im1, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Difference\n",
    "im2 = axs[2].imshow(D_diff, cmap=\"coolwarm\")\n",
    "axs[2].set_title(\"Difference (actual - embedding)\")\n",
    "axs[2].set_xlabel(\"Pair idx\")\n",
    "axs[2].set_ylabel(\"Pair idx\")\n",
    "plt.colorbar(im2, ax=axs[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Distance matrices at step {step_idx+1}\\n\"\n",
    "    f\"Kruskal stress-1: {stress1:.3f}, kstress: {kst:.3f}, r: {r:.3f}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e996739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair RDMs (Euclidean) for h1 pair embeddings across nine positions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "def load_layer_from_csv_gz(csv_gz_path):\n",
    "    df = pd.read_csv(csv_gz_path, compression='gzip')\n",
    "    seed_cols = [c for c in df.columns if c.startswith('seed_')]\n",
    "    time_steps = int(df['time_step'].max()) + 1\n",
    "    items_n = int(df['item'].max()) + 1\n",
    "    units = int(df['unit'].max()) + 1\n",
    "    seeds_n = len(seed_cols)\n",
    "    arr = np.zeros((seeds_n, time_steps, items_n, units))\n",
    "    for _, row in df.iterrows():\n",
    "        t = int(row['time_step']); i = int(row['item']); u = int(row['unit'])\n",
    "        for s_idx, sc in enumerate(seed_cols):\n",
    "            arr[s_idx, t, i, u] = row[sc]\n",
    "    return arr  # (seeds, time, items, units)\n",
    "\n",
    "def build_pair_vectors(X_items):\n",
    "    # Ordered pairs (i != j), feature = concat(X[i], X[j])\n",
    "    vecs, labels = [], []\n",
    "    n = X_items.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "            vecs.append(v)\n",
    "            labels.append(f\"({i},{j})\")\n",
    "    V = np.asarray(vecs)\n",
    "    V = np.nan_to_num(V, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    V = V - V.mean(axis=0, keepdims=True)\n",
    "    return V, labels\n",
    "\n",
    "# Load h1s from gzipped CSV and average across seeds\n",
    "csv_gz_h1_path = r\"C:\\Users\\lukel\\iCloudDrive\\Computer\\Current\\0. Columbia\\Research\\Nelli Reimplementation\\conjunctive_lazy_rich_1e-5\\conjunctive_lazy_rich_h1s.csv.gz\"\n",
    "H1 = load_layer_from_csv_gz(csv_gz_h1_path).mean(axis=0)  # (time, items, h1)\n",
    "items_n = H1.shape[1]\n",
    "steps_after = H1.shape[0] - 1\n",
    "\n",
    "# Same nine positions as above\n",
    "positions = [0.00, 0.10, 0.20, 0.50, 0.65, 0.75, 0.85, 0.90, 1.00]\n",
    "\n",
    "# Find shared vmax across selected steps for consistent color scale\n",
    "vmax = 0.0\n",
    "for p in positions:\n",
    "    t_idx = max(0, int(p * steps_after))\n",
    "    X_items = np.nan_to_num(H1[t_idx], nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    V, _ = build_pair_vectors(X_items)\n",
    "    D = euclidean_distances(V)\n",
    "    vmax = max(vmax, float(np.ceil(np.max(D) * 10) / 10.0))\n",
    "if vmax == 0.0:\n",
    "    vmax = 1.0\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    t_idx = max(0, int(p * steps_after))\n",
    "    X_items = np.nan_to_num(H1[t_idx], nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    V, labels = build_pair_vectors(X_items)\n",
    "    D = euclidean_distances(V)  # RDM of pairs\n",
    "\n",
    "    ax = axs[r, c]\n",
    "    im = ax.imshow(D, cmap=plt.cm.magma, vmin=0.0, vmax=vmax, interpolation='nearest')\n",
    "    ax.set_title(f\"h1 pair RDM — step {t_idx+1}\")\n",
    "    # Too many pairs to label ticks cleanly; hide ticks for readability\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for pos in [\"top\", \"right\", \"bottom\", \"left\"]:\n",
    "        ax.spines[pos].set_linewidth(1)\n",
    "        ax.spines[pos].set_color([0.5]*3)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axs, shrink=.6, aspect=25)\n",
    "cbar.set_ticks([])\n",
    "cbar.ax.set_ylabel(\"Euclidean distance\")\n",
    "plt.show()\n",
    "\n",
    "# Write out the x axes pairings (labels) used in the RDM plots above.\n",
    "print(\"x-axis pairings for the h1 pair RDMs (plot order):\\n\")\n",
    "for idx, label in enumerate(labels):\n",
    "    print(f\"{idx}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88845d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair RDMs (Euclidean) for h2 pair embeddings across nine positions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "def load_layer_from_csv_gz(csv_gz_path):\n",
    "    df = pd.read_csv(csv_gz_path, compression='gzip')\n",
    "    seed_cols = [c for c in df.columns if c.startswith('seed_')]\n",
    "    time_steps = int(df['time_step'].max()) + 1\n",
    "    items_n = int(df['item'].max()) + 1\n",
    "    units = int(df['unit'].max()) + 1\n",
    "    seeds_n = len(seed_cols)\n",
    "    arr = np.zeros((seeds_n, time_steps, items_n, units))\n",
    "    for _, row in df.iterrows():\n",
    "        t = int(row['time_step']); i = int(row['item']); u = int(row['unit'])\n",
    "        for s_idx, sc in enumerate(seed_cols):\n",
    "            arr[s_idx, t, i, u] = row[sc]\n",
    "    return arr  # (seeds, time, items, units)\n",
    "\n",
    "def build_pair_vectors(X_items):\n",
    "    # Ordered pairs (i != j), feature = concat(X[i], X[j])\n",
    "    vecs, labels = [], []\n",
    "    n = X_items.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "            vecs.append(v)\n",
    "            labels.append(f\"({i},{j})\")\n",
    "    V = np.asarray(vecs)\n",
    "    V = np.nan_to_num(V, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    V = V - V.mean(axis=0, keepdims=True)\n",
    "    return V, labels\n",
    "\n",
    "# Load h2s from gzipped CSV and average across seeds\n",
    "csv_gz_h2_path = r\"C:\\Users\\lukel\\iCloudDrive\\Computer\\Current\\0. Columbia\\Research\\Nelli Reimplementation\\conjunctive_lazy_rich_1e-5\\conjunctive_lazy_rich_h2s.csv.gz\"\n",
    "H2 = load_layer_from_csv_gz(csv_gz_h2_path).mean(axis=0)  # (time, items, h2)\n",
    "items_n = H2.shape[1]\n",
    "steps_after = H2.shape[0] - 1\n",
    "\n",
    "# Same nine positions as above\n",
    "positions = [0.00, 0.10, 0.20, 0.50, 0.65, 0.75, 0.85, 0.90, 1.00]\n",
    "\n",
    "# Find shared vmax across selected steps for consistent color scale\n",
    "vmax = 0.0\n",
    "for p in positions:\n",
    "    t_idx = max(0, int(p * steps_after))\n",
    "    X_items = np.nan_to_num(H2[t_idx], nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    V, _ = build_pair_vectors(X_items)\n",
    "    D = euclidean_distances(V)\n",
    "    vmax = max(vmax, float(np.ceil(np.max(D) * 10) / 10.0))\n",
    "if vmax == 0.0:\n",
    "    vmax = 1.0\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    t_idx = max(0, int(p * steps_after))\n",
    "    X_items = np.nan_to_num(H2[t_idx], nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    V, labels = build_pair_vectors(X_items)\n",
    "    D = euclidean_distances(V)  # RDM of pairs\n",
    "\n",
    "    ax = axs[r, c]\n",
    "    im = ax.imshow(D, cmap=plt.cm.magma, vmin=0.0, vmax=vmax, interpolation='nearest')\n",
    "    ax.set_title(f\"h2 pair RDM — step {t_idx+1}\")\n",
    "    # Too many pairs to label ticks cleanly; hide ticks for readability\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for pos in [\"top\", \"right\", \"bottom\", \"left\"]:\n",
    "        ax.spines[pos].set_linewidth(1)\n",
    "        ax.spines[pos].set_color([0.5]*3)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axs, shrink=.6, aspect=25)\n",
    "cbar.set_ticks([])\n",
    "cbar.ax.set_ylabel(\"Euclidean distance\")\n",
    "plt.show()\n",
    "\n",
    "# Write out the x axes pairings (labels) used in the RDM plots above.\n",
    "print(\"x-axis pairings for the h2 pair RDMs (plot order):\\n\")\n",
    "for idx, label in enumerate(labels):\n",
    "    print(f\"{idx}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ca013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704c50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the highest values in the difference matrix, print the pairs (row, col) and the values.\n",
    "# Also print exactly what the pairing is in terms of items.\n",
    "\n",
    "# How many to print:\n",
    "top_n = 10\n",
    "\n",
    "# If labels is not already available, rebuild\n",
    "if 'labels' not in locals():\n",
    "    _, labels, _ = build_pairs(H2_avg[step_idx])\n",
    "\n",
    "# Get the indices of the largest absolute differences (excluding diagonal)\n",
    "D_diff_no_diag = D_diff.copy()\n",
    "np.fill_diagonal(D_diff_no_diag, 0)  # so diag is not included in top abs\n",
    "\n",
    "flat_indices = np.abs(D_diff_no_diag).ravel().argsort()[::-1]  # descending\n",
    "rows, cols = np.unravel_index(flat_indices, D_diff.shape)\n",
    "\n",
    "printed = 0\n",
    "seen = set()\n",
    "print(\"Top differences between actual and embedded distances:\")\n",
    "for r, c in zip(rows, cols):\n",
    "    if r == c:\n",
    "        continue  # skip diagonal\n",
    "    # Show each unordered pair just once:\n",
    "    if (c, r) in seen or (r, c) in seen: \n",
    "        continue\n",
    "    # Try to retrieve the actual underlying items for these rows and cols, using the pair labels\n",
    "    label_r = labels[r] if r < len(labels) else f\"({r} out of bounds)\"\n",
    "    label_c = labels[c] if c < len(labels) else f\"({c} out of bounds)\"\n",
    "    print(\n",
    "        f\"Pair ({r}, {c}): {D_diff[r, c]:.5f} | actual={D_actual[r, c]:.5f} | embedded={D_embedded[r, c]:.5f}\\n\"\n",
    "        f\"    Pairing: {label_r} vs {label_c}\"\n",
    "    )\n",
    "    seen.add((r, c))\n",
    "    printed += 1\n",
    "    if printed >= top_n:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "import pandas as pd\n",
    "\n",
    "# Get H2_avg (time_steps, items_n, h2_size) from CSV if not already present\n",
    "if 'all_h2s' in globals():\n",
    "    H2_avg = all_h2s\n",
    "else:\n",
    "    def load_h2_mean_from_csv_gz(csv_gz_path):\n",
    "        df = pd.read_csv(csv_gz_path, compression='gzip')\n",
    "        seed_cols = [c for c in df.columns if c.startswith('seed_')]\n",
    "        grouped = df.groupby(['time_step', 'item', 'unit'])[seed_cols].mean()\n",
    "        mean_series = grouped.mean(axis=1)\n",
    "        T = int(df['time_step'].max()) + 1\n",
    "        I = int(df['item'].max()) + 1\n",
    "        U = int(df['unit'].max()) + 1\n",
    "        arr = np.zeros((T, I, U))\n",
    "        for (t, i, u), val in mean_series.items():\n",
    "            arr[int(t), int(i), int(u)] = float(val)\n",
    "        return arr\n",
    "    H2_avg = load_h2_mean_from_csv_gz('conjunctive_lazy_rich_h2s.csv.gz')\n",
    "\n",
    "time_steps, items_n_inferred, h2_size = H2_avg.shape\n",
    "step_a = min(1500, time_steps - 1)\n",
    "step_b = min(1950, time_steps - 1)\n",
    "seed = mds_seed if 'mds_seed' in globals() else 0\n",
    "\n",
    "def build_pairs(X_items):\n",
    "    vecs, labels = [], []\n",
    "    for i in range(items_n_inferred):\n",
    "        for j in range(items_n_inferred):\n",
    "            if i == j:\n",
    "                continue\n",
    "            v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "            vecs.append(v)\n",
    "            labels.append(f\"({i},{j})\")\n",
    "    V = np.asarray(vecs)\n",
    "    V = V - V.mean(axis=0, keepdims=True)\n",
    "    return V, labels\n",
    "\n",
    "Va, la = build_pairs(H2_avg[step_a])\n",
    "Vb, lb = build_pairs(H2_avg[step_b])\n",
    "assert la == lb\n",
    "\n",
    "Da = euclidean_distances(Va)\n",
    "Db = euclidean_distances(Vb)\n",
    "\n",
    "# Independent 2D MDS per step\n",
    "mds = manifold.MDS(n_components=2, dissimilarity='precomputed', random_state=seed, n_init=4, normalized_stress='auto')\n",
    "A = mds.fit(Da).embedding_\n",
    "B = manifold.MDS(n_components=2, dissimilarity='precomputed', random_state=seed, n_init=4, normalized_stress='auto').fit(Db).embedding_\n",
    "\n",
    "# Procrustes-align B to A to remove rotation/scale/translation\n",
    "A0 = A - A.mean(axis=0, keepdims=True)\n",
    "B0 = B - B.mean(axis=0, keepdims=True)\n",
    "R, _ = orthogonal_procrustes(B0, A0)\n",
    "BR = B0 @ R\n",
    "s = (A0 * BR).sum() / (BR * BR).sum()\n",
    "B_aligned = s * BR + A.mean(axis=0, keepdims=True)\n",
    "\n",
    "# 2D interpoint distances\n",
    "da2 = euclidean_distances(A)\n",
    "db2 = euclidean_distances(B_aligned)\n",
    "\n",
    "# Top movers by aligned 2D displacement\n",
    "disp = np.linalg.norm(B_aligned - A, axis=1)\n",
    "top_k = 6\n",
    "top_idx = np.argsort(-disp)[:top_k]\n",
    "\n",
    "# Helper: correlation and RMSE between HD and 2D distances for a focal point p\n",
    "def fit_quality_for_point(p, D_hd, D_2d):\n",
    "    mask = np.ones(D_hd.shape[0], dtype=bool)\n",
    "    mask[p] = False\n",
    "    x = D_hd[p, mask]\n",
    "    y = D_2d[p, mask]\n",
    "    # Guard against all-zeros\n",
    "    if np.allclose(x.std(), 0) or np.allclose(y.std(), 0):\n",
    "        r = np.nan\n",
    "    else:\n",
    "        r = np.corrcoef(x, y)[0, 1]\n",
    "    rmse = np.sqrt(np.mean((x - y) ** 2))\n",
    "    return r, rmse, x, y\n",
    "\n",
    "# Plot: for each top-moving point, HD vs 2D distances at step A and B\n",
    "rows = top_k\n",
    "fig, axes = plt.subplots(rows, 2, figsize=(10, 3.2 * rows), constrained_layout=True)\n",
    "axes = np.atleast_2d(axes)\n",
    "\n",
    "for r, p in enumerate(top_idx):\n",
    "    rA, rmA, xA, yA = fit_quality_for_point(p, Da, da2)\n",
    "    rB, rmB, xB, yB = fit_quality_for_point(p, Db, db2)\n",
    "\n",
    "    ax = axes[r, 0]\n",
    "    ax.scatter(xA, yA, s=12, alpha=0.7)\n",
    "    lim = [0, max(xA.max(), yA.max()) * 1.05]\n",
    "    ax.plot(lim, lim, 'k--', lw=1)\n",
    "    ax.set_xlim(lim); ax.set_ylim(lim)\n",
    "    ax.set_title(f\"{la[p]} — Step {step_a}\\nr={rA:.2f}, RMSE={rmA:.3f}\")\n",
    "    ax.set_xlabel(\"HD distance\"); ax.set_ylabel(\"2D distance\")\n",
    "\n",
    "    ax = axes[r, 1]\n",
    "    ax.scatter(xB, yB, s=12, alpha=0.7, color='C1')\n",
    "    lim = [0, max(xB.max(), yB.max()) * 1.05]\n",
    "    ax.plot(lim, lim, 'k--', lw=1)\n",
    "    ax.set_xlim(lim); ax.set_ylim(lim)\n",
    "    ax.set_title(f\"{la[p]} — Step {step_b}\\nr={rB:.2f}, RMSE={rmB:.3f}\")\n",
    "    ax.set_xlabel(\"HD distance\"); ax.set_ylabel(\"2D distance\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Optional: table of 2D displacement vs true HD displacement of that point (feature space)\n",
    "hd_disp = np.linalg.norm(Vb - Va, axis=1)\n",
    "print(\"pair_label, 2D_disp, HD_feature_disp\")\n",
    "for p in top_idx:\n",
    "    print(f\"{la[p]}, {disp[p]:.3f}, {hd_disp[p]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save r1s and r2s (all seeds) every 10 steps to CSV and gzip\n",
    "import csv, gzip, shutil\n",
    "import numpy as np\n",
    "\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "train = results.get(\"train\", {})\n",
    "\n",
    "def save_layer_csv_gz(layer_key, base_filename, step_stride=10):\n",
    "    arr = train.get(layer_key)\n",
    "    if arr is None or arr.ndim != 4:\n",
    "        print(f\"{layer_key} missing or wrong shape; expected (seeds, time, items, units).\")\n",
    "        return\n",
    "    seeds_n, time_steps, items_n, units = arr.shape\n",
    "    header = [\"time_step\", \"item\", \"unit\"] + [f\"seed_{s}\" for s in range(seeds_n)] + [\"mean_activation\", \"std_activation\"]\n",
    "    rows = []\n",
    "    for t in range(0, time_steps, step_stride):\n",
    "        for i in range(items_n):\n",
    "            for u in range(units):\n",
    "                vals = arr[:, t, i, u]\n",
    "                rows.append([t, i, u] + list(vals) + [float(vals.mean()), float(vals.std())])\n",
    "    csv_filename = f\"{base_filename}.csv\"\n",
    "    with open(csv_filename, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f); w.writerow(header); w.writerows(rows)\n",
    "    gz_filename = csv_filename + \".gz\"\n",
    "    with open(csv_filename, \"rb\") as fin, gzip.open(gz_filename, \"wb\") as fout:\n",
    "        shutil.copyfileobj(fin, fout)\n",
    "    print(f\"{layer_key} saved to {csv_filename} (gzipped to {gz_filename})\")\n",
    "\n",
    "save_layer_csv_gz(\"r1s\", \"conjunctive_lazy_rich_r1s\", step_stride=10)\n",
    "save_layer_csv_gz(\"r2s\", \"conjunctive_lazy_rich_r2s\", step_stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf86892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot r1 and r2 scalar pair scatters (x=r(i), y=r(j)) across nine positions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_r_from_csv_gz(csv_gz_path):\n",
    "    df = pd.read_csv(csv_gz_path, compression=\"gzip\")\n",
    "    seed_cols = [c for c in df.columns if c.startswith(\"seed_\")]\n",
    "    steps = np.array(sorted(df[\"time_step\"].unique()))\n",
    "    items_n = int(df[\"item\"].max()) + 1\n",
    "    units = int(df[\"unit\"].max()) + 1\n",
    "    seeds_n = len(seed_cols)\n",
    "    arr = np.zeros((seeds_n, len(steps), items_n, units), dtype=float)\n",
    "    step_to_idx = {s: k for k, s in enumerate(steps)}\n",
    "    for _, row in df.iterrows():\n",
    "        t = int(row[\"time_step\"]); i = int(row[\"item\"]); u = int(row[\"unit\"])\n",
    "        k = step_to_idx[t]\n",
    "        for s_idx, sc in enumerate(seed_cols):\n",
    "            arr[s_idx, k, i, u] = row[sc]\n",
    "    return arr, steps  # (seeds, saved_steps, items, units), saved step values\n",
    "\n",
    "# Paths (adjust if needed)\n",
    "r1_csv_gz = r\"conjunctive_lazy_rich_r1s.csv.gz\"\n",
    "r2_csv_gz = r\"conjunctive_lazy_rich_r2s.csv.gz\"\n",
    "\n",
    "R1_all, steps_r1 = load_r_from_csv_gz(r1_csv_gz)\n",
    "R2_all, steps_r2 = load_r_from_csv_gz(r2_csv_gz)\n",
    "R1 = R1_all.mean(axis=0)  # (K, items, units)\n",
    "R2 = R2_all.mean(axis=0)  # (K, items, units)\n",
    "\n",
    "# If units>1, take unit 0 (r1/r2 should be scalar)\n",
    "R1_scalar = R1[..., 0]\n",
    "R2_scalar = R2[..., 0]\n",
    "items_n = R1_scalar.shape[1]\n",
    "\n",
    "positions = [0.00, 0.10, 0.20, 0.50, 0.65, 0.75, 0.85, 0.90, 1.00]\n",
    "def nearest_index(saved_steps, target_step):\n",
    "    idx = int(np.argmin(np.abs(saved_steps - target_step)))\n",
    "    return idx, int(saved_steps[idx])\n",
    "\n",
    "# r1 pair scatter grids\n",
    "max_step_r1 = int(steps_r1.max())\n",
    "fig1, axs1 = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "fig1.patch.set_facecolor('white')\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    tgt = int(p * max_step_r1)\n",
    "    k, actual = nearest_index(steps_r1, tgt)\n",
    "    rvals = R1_scalar[k]  # (items,)\n",
    "    xs, ys = [], []\n",
    "    for i in range(items_n):\n",
    "        for j in range(items_n):\n",
    "            if i == j: \n",
    "                continue\n",
    "            xs.append(rvals[i]); ys.append(rvals[j])\n",
    "    ax = axs1[r, c]\n",
    "    ax.scatter(xs, ys, s=10, color='k', alpha=0.7)\n",
    "    ax.plot([min(xs+ys), max(xs+ys)], [min(xs+ys), max(xs+ys)], color='lightgray', lw=1)  # y=x\n",
    "    ax.set_title(f\"r1 scalar pairs — step {actual}\")\n",
    "    ax.set_xlabel(\"r1(i)\"); ax.set_ylabel(\"r1(j)\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# r2 pair scatter grids\n",
    "max_step_r2 = int(steps_r2.max())\n",
    "fig2, axs2 = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "fig2.patch.set_facecolor('white')\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    tgt = int(p * max_step_r2)\n",
    "    k, actual = nearest_index(steps_r2, tgt)\n",
    "    rvals = R2_scalar[k]  # (items,)\n",
    "    xs, ys = [], []\n",
    "    for i in range(items_n):\n",
    "        for j in range(items_n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            xs.append(rvals[i]); ys.append(rvals[j])\n",
    "    ax = axs2[r, c]\n",
    "    ax.scatter(xs, ys, s=10, color='k', alpha=0.7)\n",
    "    ax.plot([min(xs+ys), max(xs+ys)], [min(xs+ys), max(xs+ys)], color='lightgray', lw=1)\n",
    "    ax.set_title(f\"r2 scalar pairs — step {actual}\")\n",
    "    ax.set_xlabel(\"r2(i)\"); ax.set_ylabel(\"r2(j)\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load r1s/r2s from CSV.GZ (every 10 steps) and plot pair PCA embeddings across nine positions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def load_r_from_csv_gz(csv_gz_path):\n",
    "    df = pd.read_csv(csv_gz_path, compression=\"gzip\")\n",
    "    seed_cols = [c for c in df.columns if c.startswith(\"seed_\")]\n",
    "    steps = np.array(sorted(df[\"time_step\"].unique()))\n",
    "    items_n = int(df[\"item\"].max()) + 1\n",
    "    units = int(df[\"unit\"].max()) + 1\n",
    "    seeds_n = len(seed_cols)\n",
    "    arr = np.zeros((seeds_n, len(steps), items_n, units), dtype=float)\n",
    "    step_to_idx = {s: k for k, s in enumerate(steps)}\n",
    "    for _, row in df.iterrows():\n",
    "        t = int(row[\"time_step\"]); i = int(row[\"item\"]); u = int(row[\"unit\"])\n",
    "        k = step_to_idx[t]\n",
    "        for s_idx, sc in enumerate(seed_cols):\n",
    "            arr[s_idx, k, i, u] = row[sc]\n",
    "    return arr, steps  # (seeds, saved_steps, items, units), saved step values\n",
    "\n",
    "def build_pair_vectors(X_items):\n",
    "    # Ordered pairs (i != j) with concatenated readout vectors\n",
    "    n = X_items.shape[0]\n",
    "    vecs, labels = [], []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            vecs.append(np.concatenate([X_items[i], X_items[j]], axis=0))\n",
    "            labels.append(f\"({i},{j})\")\n",
    "    V = np.asarray(vecs, dtype=float)\n",
    "    V = np.nan_to_num(V, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    V -= V.mean(axis=0, keepdims=True)\n",
    "    return V, labels\n",
    "\n",
    "def pca2(V, seed=0):\n",
    "    if np.allclose(V.var(), 0.0):\n",
    "        theta = np.linspace(0, 2*np.pi, V.shape[0], endpoint=False)\n",
    "        return np.stack([np.cos(theta), np.sin(theta)], axis=1)\n",
    "    k = min(2, V.shape[1])\n",
    "    return PCA(n_components=k, random_state=seed).fit_transform(V)\n",
    "\n",
    "# Paths (adjust if saving elsewhere)\n",
    "r1_csv_gz = r\"conjunctive_lazy_rich_r1s.csv.gz\"\n",
    "r2_csv_gz = r\"conjunctive_lazy_rich_r2s.csv.gz\"\n",
    "\n",
    "R1_all, steps_r1 = load_r_from_csv_gz(r1_csv_gz)  # (seeds, K, items, readouts)\n",
    "R2_all, steps_r2 = load_r_from_csv_gz(r2_csv_gz)  # (seeds, K, items, readouts)\n",
    "R1 = R1_all.mean(axis=0)  # (K, items, readouts)\n",
    "R2 = R2_all.mean(axis=0)  # (K, items, readouts)\n",
    "items_n = R1.shape[1]\n",
    "mds_seed = 0\n",
    "\n",
    "# Positions as fractions of max step; map to nearest saved step\n",
    "positions = [0.00, 0.10, 0.20, 0.50, 0.65, 0.75, 0.85, 0.90, 1.00]\n",
    "max_step_r1 = int(steps_r1.max())\n",
    "def nearest_index(saved_steps, target_step):\n",
    "    idx = np.argmin(np.abs(saved_steps - target_step))\n",
    "    return int(idx), int(saved_steps[idx])\n",
    "\n",
    "# Plot r1 pairs\n",
    "fig1, axs1 = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "fig1.patch.set_facecolor('white')\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    tgt = int(p * max_step_r1)\n",
    "    k, actual = nearest_index(steps_r1, tgt)\n",
    "    V1, L1 = build_pair_vectors(R1[k])\n",
    "    coords1 = pca2(V1, seed=mds_seed)\n",
    "    ax = axs1[r, c]\n",
    "    ax.scatter(coords1[:, 0], coords1[:, 1], s=10, color='k', alpha=0.7)\n",
    "    ax.set_title(f\"r1 pairs — step {actual}\")\n",
    "    ax.axhline(0, color='lightgray', lw=1); ax.axvline(0, color='lightgray', lw=1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# Plot r2 pairs\n",
    "max_step_r2 = int(steps_r2.max())\n",
    "fig2, axs2 = plt.subplots(3, 3, figsize=(12., 12.), constrained_layout=True)\n",
    "fig2.patch.set_facecolor('white')\n",
    "for idx, p in enumerate(positions):\n",
    "    r, c = divmod(idx, 3)\n",
    "    tgt = int(p * max_step_r2)\n",
    "    k, actual = nearest_index(steps_r2, tgt)\n",
    "    V2, L2 = build_pair_vectors(R2[k])\n",
    "    coords2 = pca2(V2, seed=mds_seed)\n",
    "    ax = axs2[r, c]\n",
    "    ax.scatter(coords2[:, 0], coords2[:, 1], s=10, color='k', alpha=0.7)\n",
    "    ax.set_title(f\"r2 pairs — step {actual}\")\n",
    "    ax.axhline(0, color='lightgray', lw=1); ax.axvline(0, color='lightgray', lw=1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aff36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 pair changes between steps 1500 and 1950: show top-5 moving pairs with arrows\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare h2 data (mean across seeds)\n",
    "H2_avg = all_h2s\n",
    "time_steps, items_n_inferred, h2_size = H2_avg.shape\n",
    "\n",
    "# Steps of interest (clip to available range)\n",
    "step_a = min(1500, time_steps - 1)\n",
    "step_b = min(1950, time_steps - 1)\n",
    "\n",
    "# Build pair feature vectors by concatenation for all ordered pairs i != j\n",
    "def build_pairs(X_items):\n",
    "    vecs, labels, pairs = [], [], []\n",
    "    for i in range(items_n_inferred):\n",
    "        for j in range(items_n_inferred):\n",
    "            if i == j:\n",
    "                continue\n",
    "            v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "            vecs.append(v)\n",
    "            labels.append(f\"({i},{j})\")\n",
    "            pairs.append((i, j))\n",
    "    V = np.asarray(vecs)\n",
    "    V = V - V.mean(axis=0, keepdims=True)\n",
    "    return V, labels, pairs\n",
    "\n",
    "Xa, la, pa = build_pairs(H2_avg[step_a])\n",
    "Xb, lb, pb = build_pairs(H2_avg[step_b])\n",
    "\n",
    "# Fit PCA on the union to get a common 2D space\n",
    "X_union = np.vstack([Xa, Xb])\n",
    "if np.allclose(X_union.var(), 0.0):\n",
    "    theta = np.linspace(0, 2*np.pi, Xa.shape[0], endpoint=False)\n",
    "    coords_a = np.stack([np.cos(theta), np.sin(theta)], axis=1)\n",
    "    coords_b = coords_a.copy()\n",
    "else:\n",
    "    pca = PCA(n_components=2, random_state=mds_seed if 'mds_seed' in globals() else 0)\n",
    "    coords_union = pca.fit_transform(X_union)\n",
    "    coords_a = coords_union[:Xa.shape[0]]\n",
    "    coords_b = coords_union[Xa.shape[0]:]\n",
    "\n",
    "# Compute displacements per pair (match by index; la and lb have same ordering)\n",
    "deltas = coords_b - coords_a\n",
    "move_norm = np.linalg.norm(deltas, axis=1)\n",
    "\n",
    "# Select top-5 moving pairs\n",
    "top_k = 10\n",
    "top_idx = np.argsort(-move_norm)[:top_k]\n",
    "\n",
    "# Plot arrows for the top pairs\n",
    "plt.figure(figsize=(7, 6))\n",
    "ax = plt.gca()\n",
    "for idx in top_idx:\n",
    "    x0, y0 = coords_a[idx]\n",
    "    dx, dy = deltas[idx]\n",
    "    x1, y1 = x0 + dx, y0 + dy\n",
    "    ax.arrow(x0, y0, dx, dy, head_width=0.03, head_length=0.05, length_includes_head=True,\n",
    "                fc='C0', ec='C0', alpha=0.9)\n",
    "    ax.scatter([x0, x1], [y0, y1], s=20, color=['C1','C2'], zorder=3)\n",
    "    ax.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "ax.set_title(f\"h2 pairs movement: step {step_a} → {step_b} (top {top_k})\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.axhline(0, color='lightgray', linewidth=1)\n",
    "ax.axvline(0, color='lightgray', linewidth=1)\n",
    "ax.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also provide separate scatter plots for the same 5 pairs at each step (optional clarity)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10., 5.), constrained_layout=True)\n",
    "ax1.set_title(f\"h2 pairs at step {step_a}\")\n",
    "ax2.set_title(f\"h2 pairs at step {step_b}\")\n",
    "for idx in top_idx:\n",
    "    x0, y0 = coords_a[idx]\n",
    "    x1, y1 = coords_b[idx]\n",
    "    ax1.scatter(x0, y0, s=25, color='k')\n",
    "    ax1.text(x0, y0, la[idx], fontsize=9, ha='left', va='center')\n",
    "    ax2.scatter(x1, y1, s=25, color='k')\n",
    "    ax2.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "for axx in (ax1, ax2):\n",
    "    axx.axhline(0, color='lightgray', linewidth=1)\n",
    "    axx.axvline(0, color='lightgray', linewidth=1)\n",
    "    axx.grid(True, alpha=0.2)\n",
    "    axx.set_xlabel(\"PC1\")\n",
    "    axx.set_ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 pair changes between steps 1500 and 1950: show top-5 moving pairs with arrows\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare h2 data (mean across seeds)\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "train_dict = results.get(\"train\", {}) if isinstance(results, dict) else {}\n",
    "if \"h2s\" not in train_dict or train_dict[\"h2s\"] is None:\n",
    "    print(\"No h2s found in results['train'].\")\n",
    "else:\n",
    "    H2 = train_dict[\"h2s\"]  # (seeds_n, time_steps, items_n, h2_size)\n",
    "    H2_avg = np.nan_to_num(H2.mean(axis=0), nan=0.0, posinf=0.0, neginf=0.0)  # (time_steps, items_n, h2_size)\n",
    "    time_steps, items_n_inferred, h2_size = H2_avg.shape\n",
    "\n",
    "    # Steps of interest (clip to available range)\n",
    "    step_a = min(1500, time_steps - 1)\n",
    "    step_b = min(1950, time_steps - 1)\n",
    "\n",
    "    # Build pair feature vectors by concatenation for all ordered pairs i != j\n",
    "    def build_pairs(X_items):\n",
    "        vecs, labels, pairs = [], [], []\n",
    "        for i in range(items_n_inferred):\n",
    "            for j in range(items_n_inferred):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "                vecs.append(v)\n",
    "                labels.append(f\"({i},{j})\")\n",
    "                pairs.append((i, j))\n",
    "        V = np.asarray(vecs)\n",
    "        V = V - V.mean(axis=0, keepdims=True)\n",
    "        return V, labels, pairs\n",
    "\n",
    "    Xa, la, pa = build_pairs(H2_avg[step_a])\n",
    "    Xb, lb, pb = build_pairs(H2_avg[step_b])\n",
    "\n",
    "    # Fit PCA on the union to get a common 2D space\n",
    "    X_union = np.vstack([Xa, Xb])\n",
    "    if np.allclose(X_union.var(), 0.0):\n",
    "        theta = np.linspace(0, 2*np.pi, Xa.shape[0], endpoint=False)\n",
    "        coords_a = np.stack([np.cos(theta), np.sin(theta)], axis=1)\n",
    "        coords_b = coords_a.copy()\n",
    "    else:\n",
    "        pca = PCA(n_components=2, random_state=mds_seed if 'mds_seed' in globals() else 0)\n",
    "        coords_union = pca.fit_transform(X_union)\n",
    "        coords_a = coords_union[:Xa.shape[0]]\n",
    "        coords_b = coords_union[Xa.shape[0]:]\n",
    "\n",
    "    # Compute displacements per pair (match by index; la and lb have same ordering)\n",
    "    deltas = coords_b - coords_a\n",
    "    move_norm = np.linalg.norm(deltas, axis=1)\n",
    "\n",
    "    # Select top-5 moving pairs\n",
    "    top_k = 10\n",
    "    top_idx = np.argsort(-move_norm)[:top_k]\n",
    "\n",
    "    # Plot arrows for the top pairs\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    ax = plt.gca()\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        dx, dy = deltas[idx]\n",
    "        x1, y1 = x0 + dx, y0 + dy\n",
    "        ax.arrow(x0, y0, dx, dy, head_width=0.03, head_length=0.05, length_includes_head=True,\n",
    "                 fc='C0', ec='C0', alpha=0.9)\n",
    "        ax.scatter([x0, x1], [y0, y1], s=20, color=['C1','C2'], zorder=3)\n",
    "        ax.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    ax.set_title(f\"h2 pairs movement: step {step_a} → {step_b} (top {top_k})\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.axhline(0, color='lightgray', linewidth=1)\n",
    "    ax.axvline(0, color='lightgray', linewidth=1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Also provide separate scatter plots for the same 5 pairs at each step (optional clarity)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10., 5.), constrained_layout=True)\n",
    "    ax1.set_title(f\"h2 pairs at step {step_a}\")\n",
    "    ax2.set_title(f\"h2 pairs at step {step_b}\")\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        x1, y1 = coords_b[idx]\n",
    "        ax1.scatter(x0, y0, s=25, color='k')\n",
    "        ax1.text(x0, y0, la[idx], fontsize=9, ha='left', va='center')\n",
    "        ax2.scatter(x1, y1, s=25, color='k')\n",
    "        ax2.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    for axx in (ax1, ax2):\n",
    "        axx.axhline(0, color='lightgray', linewidth=1)\n",
    "        axx.axvline(0, color='lightgray', linewidth=1)\n",
    "        axx.grid(True, alpha=0.2)\n",
    "        axx.set_xlabel(\"PC1\")\n",
    "        axx.set_ylabel(\"PC2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 pair changes between steps 1500 and 1950: show top-5 moving pairs with arrows\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare h2 data (mean across seeds)\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "train_dict = results.get(\"train\", {}) if isinstance(results, dict) else {}\n",
    "if \"h2s\" not in train_dict or train_dict[\"h2s\"] is None:\n",
    "    print(\"No h2s found in results['train'].\")\n",
    "else:\n",
    "    H2 = train_dict[\"h2s\"]  # (seeds_n, time_steps, items_n, h2_size)\n",
    "    H2_avg = np.nan_to_num(H2.mean(axis=0), nan=0.0, posinf=0.0, neginf=0.0)  # (time_steps, items_n, h2_size)\n",
    "    time_steps, items_n_inferred, h2_size = H2_avg.shape\n",
    "\n",
    "    # Steps of interest (clip to available range)\n",
    "    step_a = min(1500, time_steps - 1)\n",
    "    step_b = min(1950, time_steps - 1)\n",
    "\n",
    "    # Build pair feature vectors by concatenation for all ordered pairs i != j\n",
    "    def build_pairs(X_items):\n",
    "        vecs, labels, pairs = [], [], []\n",
    "        for i in range(items_n_inferred):\n",
    "            for j in range(items_n_inferred):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "                vecs.append(v)\n",
    "                labels.append(f\"({i},{j})\")\n",
    "                pairs.append((i, j))\n",
    "        V = np.asarray(vecs)\n",
    "        V = V - V.mean(axis=0, keepdims=True)\n",
    "        return V, labels, pairs\n",
    "\n",
    "    Xa, la, pa = build_pairs(H2_avg[step_a])\n",
    "    Xb, lb, pb = build_pairs(H2_avg[step_b])\n",
    "\n",
    "    # Fit PCA on the union to get a common 2D space\n",
    "    X_union = np.vstack([Xa, Xb])\n",
    "    if np.allclose(X_union.var(), 0.0):\n",
    "        theta = np.linspace(0, 2*np.pi, Xa.shape[0], endpoint=False)\n",
    "        coords_a = np.stack([np.cos(theta), np.sin(theta)], axis=1)\n",
    "        coords_b = coords_a.copy()\n",
    "    else:\n",
    "        pca = PCA(n_components=2, random_state=mds_seed if 'mds_seed' in globals() else 0)\n",
    "        coords_union = pca.fit_transform(X_union)\n",
    "        coords_a = coords_union[:Xa.shape[0]]\n",
    "        coords_b = coords_union[Xa.shape[0]:]\n",
    "\n",
    "    # Compute displacements per pair (match by index; la and lb have same ordering)\n",
    "    deltas = coords_b - coords_a\n",
    "    move_norm = np.linalg.norm(deltas, axis=1)\n",
    "\n",
    "    # Select top-5 moving pairs\n",
    "    top_k = 10\n",
    "    top_idx = np.argsort(-move_norm)[:top_k]\n",
    "\n",
    "    # Plot arrows for the top pairs\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    ax = plt.gca()\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        dx, dy = deltas[idx]\n",
    "        x1, y1 = x0 + dx, y0 + dy\n",
    "        ax.arrow(x0, y0, dx, dy, head_width=0.03, head_length=0.05, length_includes_head=True,\n",
    "                 fc='C0', ec='C0', alpha=0.9)\n",
    "        ax.scatter([x0, x1], [y0, y1], s=20, color=['C1','C2'], zorder=3)\n",
    "        ax.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    ax.set_title(f\"h2 pairs movement: step {step_a} → {step_b} (top {top_k})\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.axhline(0, color='lightgray', linewidth=1)\n",
    "    ax.axvline(0, color='lightgray', linewidth=1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Also provide separate scatter plots for the same 5 pairs at each step (optional clarity)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10., 5.), constrained_layout=True)\n",
    "    ax1.set_title(f\"h2 pairs at step {step_a}\")\n",
    "    ax2.set_title(f\"h2 pairs at step {step_b}\")\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        x1, y1 = coords_b[idx]\n",
    "        ax1.scatter(x0, y0, s=25, color='k')\n",
    "        ax1.text(x0, y0, la[idx], fontsize=9, ha='left', va='center')\n",
    "        ax2.scatter(x1, y1, s=25, color='k')\n",
    "        ax2.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    for axx in (ax1, ax2):\n",
    "        axx.axhline(0, color='lightgray', linewidth=1)\n",
    "        axx.axvline(0, color='lightgray', linewidth=1)\n",
    "        axx.grid(True, alpha=0.2)\n",
    "        axx.set_xlabel(\"PC1\")\n",
    "        axx.set_ylabel(\"PC2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afedf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 pair changes between steps 1500 and 1950: show top-5 moving pairs with arrows\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare h2 data (mean across seeds)\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "train_dict = results.get(\"train\", {}) if isinstance(results, dict) else {}\n",
    "if \"h2s\" not in train_dict or train_dict[\"h2s\"] is None:\n",
    "    print(\"No h2s found in results['train'].\")\n",
    "else:\n",
    "    H2 = train_dict[\"h2s\"]  # (seeds_n, time_steps, items_n, h2_size)\n",
    "    H2_avg = np.nan_to_num(H2.mean(axis=0), nan=0.0, posinf=0.0, neginf=0.0)  # (time_steps, items_n, h2_size)\n",
    "    time_steps, items_n_inferred, h2_size = H2_avg.shape\n",
    "\n",
    "    # Steps of interest (clip to available range)\n",
    "    step_a = min(1950, time_steps - 1)\n",
    "    step_b = min(2250, time_steps - 1)\n",
    "\n",
    "    # Build pair feature vectors by concatenation for all ordered pairs i != j\n",
    "    def build_pairs(X_items):\n",
    "        vecs, labels, pairs = [], [], []\n",
    "        for i in range(items_n_inferred):\n",
    "            for j in range(items_n_inferred):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "                vecs.append(v)\n",
    "                labels.append(f\"({i},{j})\")\n",
    "                pairs.append((i, j))\n",
    "        V = np.asarray(vecs)\n",
    "        V = V - V.mean(axis=0, keepdims=True)\n",
    "        return V, labels, pairs\n",
    "\n",
    "    Xa, la, pa = build_pairs(H2_avg[step_a])\n",
    "    Xb, lb, pb = build_pairs(H2_avg[step_b])\n",
    "\n",
    "    # Fit PCA on the union to get a common 2D space\n",
    "    X_union = np.vstack([Xa, Xb])\n",
    "    if np.allclose(X_union.var(), 0.0):\n",
    "        theta = np.linspace(0, 2*np.pi, Xa.shape[0], endpoint=False)\n",
    "        coords_a = np.stack([np.cos(theta), np.sin(theta)], axis=1)\n",
    "        coords_b = coords_a.copy()\n",
    "    else:\n",
    "        pca = PCA(n_components=2, random_state=mds_seed if 'mds_seed' in globals() else 0)\n",
    "        coords_union = pca.fit_transform(X_union)\n",
    "        coords_a = coords_union[:Xa.shape[0]]\n",
    "        coords_b = coords_union[Xa.shape[0]:]\n",
    "\n",
    "    # Compute displacements per pair (match by index; la and lb have same ordering)\n",
    "    deltas = coords_b - coords_a\n",
    "    move_norm = np.linalg.norm(deltas, axis=1)\n",
    "\n",
    "    # Select top-5 moving pairs\n",
    "    top_k = 10\n",
    "    top_idx = np.argsort(-move_norm)[:top_k]\n",
    "\n",
    "    # Plot arrows for the top pairs\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    ax = plt.gca()\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        dx, dy = deltas[idx]\n",
    "        x1, y1 = x0 + dx, y0 + dy\n",
    "        ax.arrow(x0, y0, dx, dy, head_width=0.03, head_length=0.05, length_includes_head=True,\n",
    "                 fc='C0', ec='C0', alpha=0.9)\n",
    "        ax.scatter([x0, x1], [y0, y1], s=20, color=['C1','C2'], zorder=3)\n",
    "        ax.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    ax.set_title(f\"h2 pairs movement: step {step_a} → {step_b} (top {top_k})\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.axhline(0, color='lightgray', linewidth=1)\n",
    "    ax.axvline(0, color='lightgray', linewidth=1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Also provide separate scatter plots for the same 5 pairs at each step (optional clarity)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10., 5.), constrained_layout=True)\n",
    "    ax1.set_title(f\"h2 pairs at step {step_a}\")\n",
    "    ax2.set_title(f\"h2 pairs at step {step_b}\")\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        x1, y1 = coords_b[idx]\n",
    "        ax1.scatter(x0, y0, s=25, color='k')\n",
    "        ax1.text(x0, y0, la[idx], fontsize=9, ha='left', va='center')\n",
    "        ax2.scatter(x1, y1, s=25, color='k')\n",
    "        ax2.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    for axx in (ax1, ax2):\n",
    "        axx.axhline(0, color='lightgray', linewidth=1)\n",
    "        axx.axvline(0, color='lightgray', linewidth=1)\n",
    "        axx.grid(True, alpha=0.2)\n",
    "        axx.set_xlabel(\"PC1\")\n",
    "        axx.set_ylabel(\"PC2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 pair changes between steps 1500 and 1950: show top-5 moving pairs with arrows\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare h2 data (mean across seeds)\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "train_dict = results.get(\"train\", {}) if isinstance(results, dict) else {}\n",
    "if \"h2s\" not in train_dict or train_dict[\"h2s\"] is None:\n",
    "    print(\"No h2s found in results['train'].\")\n",
    "else:\n",
    "    H2 = train_dict[\"h2s\"]  # (seeds_n, time_steps, items_n, h2_size)\n",
    "    H2_avg = np.nan_to_num(H2.mean(axis=0), nan=0.0, posinf=0.0, neginf=0.0)  # (time_steps, items_n, h2_size)\n",
    "    time_steps, items_n_inferred, h2_size = H2_avg.shape\n",
    "\n",
    "    # Steps of interest (clip to available range)\n",
    "    step_a = min(1500, time_steps - 1)\n",
    "    step_b = min(3000, time_steps - 1)\n",
    "\n",
    "    # Build pair feature vectors by concatenation for all ordered pairs i != j\n",
    "    def build_pairs(X_items):\n",
    "        vecs, labels, pairs = [], [], []\n",
    "        for i in range(items_n_inferred):\n",
    "            for j in range(items_n_inferred):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "                vecs.append(v)\n",
    "                labels.append(f\"({i},{j})\")\n",
    "                pairs.append((i, j))\n",
    "        V = np.asarray(vecs)\n",
    "        V = V - V.mean(axis=0, keepdims=True)\n",
    "        return V, labels, pairs\n",
    "\n",
    "    Xa, la, pa = build_pairs(H2_avg[step_a])\n",
    "    Xb, lb, pb = build_pairs(H2_avg[step_b])\n",
    "\n",
    "    # Fit PCA on the union to get a common 2D space\n",
    "    X_union = np.vstack([Xa, Xb])\n",
    "    if np.allclose(X_union.var(), 0.0):\n",
    "        theta = np.linspace(0, 2*np.pi, Xa.shape[0], endpoint=False)\n",
    "        coords_a = np.stack([np.cos(theta), np.sin(theta)], axis=1)\n",
    "        coords_b = coords_a.copy()\n",
    "    else:\n",
    "        pca = PCA(n_components=2, random_state=mds_seed if 'mds_seed' in globals() else 0)\n",
    "        coords_union = pca.fit_transform(X_union)\n",
    "        coords_a = coords_union[:Xa.shape[0]]\n",
    "        coords_b = coords_union[Xa.shape[0]:]\n",
    "\n",
    "    # Compute displacements per pair (match by index; la and lb have same ordering)\n",
    "    deltas = coords_b - coords_a\n",
    "    move_norm = np.linalg.norm(deltas, axis=1)\n",
    "\n",
    "    # Select top-5 moving pairs\n",
    "    top_k = 10\n",
    "    top_idx = np.argsort(-move_norm)[:top_k]\n",
    "\n",
    "    # Plot arrows for the top pairs\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    ax = plt.gca()\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        dx, dy = deltas[idx]\n",
    "        x1, y1 = x0 + dx, y0 + dy\n",
    "        ax.arrow(x0, y0, dx, dy, head_width=0.03, head_length=0.05, length_includes_head=True,\n",
    "                 fc='C0', ec='C0', alpha=0.9)\n",
    "        ax.scatter([x0, x1], [y0, y1], s=20, color=['C1','C2'], zorder=3)\n",
    "        ax.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    ax.set_title(f\"h2 pairs movement: step {step_a} → {step_b} (top {top_k})\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.axhline(0, color='lightgray', linewidth=1)\n",
    "    ax.axvline(0, color='lightgray', linewidth=1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Also provide separate scatter plots for the same 5 pairs at each step (optional clarity)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10., 5.), constrained_layout=True)\n",
    "    ax1.set_title(f\"h2 pairs at step {step_a}\")\n",
    "    ax2.set_title(f\"h2 pairs at step {step_b}\")\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        x1, y1 = coords_b[idx]\n",
    "        ax1.scatter(x0, y0, s=25, color='k')\n",
    "        ax1.text(x0, y0, la[idx], fontsize=9, ha='left', va='center')\n",
    "        ax2.scatter(x1, y1, s=25, color='k')\n",
    "        ax2.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    for axx in (ax1, ax2):\n",
    "        axx.axhline(0, color='lightgray', linewidth=1)\n",
    "        axx.axvline(0, color='lightgray', linewidth=1)\n",
    "        axx.grid(True, alpha=0.2)\n",
    "        axx.set_xlabel(\"PC1\")\n",
    "        axx.set_ylabel(\"PC2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 pair changes between steps 1500 and 1950: show top-5 moving pairs with arrows\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare h2 data (mean across seeds)\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "train_dict = results.get(\"train\", {}) if isinstance(results, dict) else {}\n",
    "if \"h2s\" not in train_dict or train_dict[\"h2s\"] is None:\n",
    "    print(\"No h2s found in results['train'].\")\n",
    "else:\n",
    "    H2 = train_dict[\"h2s\"]  # (seeds_n, time_steps, items_n, h2_size)\n",
    "    H2_avg = np.nan_to_num(H2.mean(axis=0), nan=0.0, posinf=0.0, neginf=0.0)  # (time_steps, items_n, h2_size)\n",
    "    time_steps, items_n_inferred, h2_size = H2_avg.shape\n",
    "\n",
    "    # Steps of interest (clip to available range)\n",
    "    step_a = min(0, time_steps - 1)\n",
    "    step_b = min(300, time_steps - 1)\n",
    "\n",
    "    # Build pair feature vectors by concatenation for all ordered pairs i != j\n",
    "    def build_pairs(X_items):\n",
    "        vecs, labels, pairs = [], [], []\n",
    "        for i in range(items_n_inferred):\n",
    "            for j in range(items_n_inferred):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "                vecs.append(v)\n",
    "                labels.append(f\"({i},{j})\")\n",
    "                pairs.append((i, j))\n",
    "        V = np.asarray(vecs)\n",
    "        V = V - V.mean(axis=0, keepdims=True)\n",
    "        return V, labels, pairs\n",
    "\n",
    "    Xa, la, pa = build_pairs(H2_avg[step_a])\n",
    "    Xb, lb, pb = build_pairs(H2_avg[step_b])\n",
    "\n",
    "    # Fit PCA on the union to get a common 2D space\n",
    "    X_union = np.vstack([Xa, Xb])\n",
    "    if np.allclose(X_union.var(), 0.0):\n",
    "        theta = np.linspace(0, 2*np.pi, Xa.shape[0], endpoint=False)\n",
    "        coords_a = np.stack([np.cos(theta), np.sin(theta)], axis=1)\n",
    "        coords_b = coords_a.copy()\n",
    "    else:\n",
    "        pca = PCA(n_components=2, random_state=mds_seed if 'mds_seed' in globals() else 0)\n",
    "        coords_union = pca.fit_transform(X_union)\n",
    "        coords_a = coords_union[:Xa.shape[0]]\n",
    "        coords_b = coords_union[Xa.shape[0]:]\n",
    "\n",
    "    # Compute displacements per pair (match by index; la and lb have same ordering)\n",
    "    deltas = coords_b - coords_a\n",
    "    move_norm = np.linalg.norm(deltas, axis=1)\n",
    "\n",
    "    # Select top-5 moving pairs\n",
    "    top_k = 10\n",
    "    top_idx = np.argsort(-move_norm)[:top_k]\n",
    "\n",
    "    # Plot arrows for the top pairs\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    ax = plt.gca()\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        dx, dy = deltas[idx]\n",
    "        x1, y1 = x0 + dx, y0 + dy\n",
    "        ax.arrow(x0, y0, dx, dy, head_width=0.03, head_length=0.05, length_includes_head=True,\n",
    "                 fc='C0', ec='C0', alpha=0.9)\n",
    "        ax.scatter([x0, x1], [y0, y1], s=20, color=['C1','C2'], zorder=3)\n",
    "        ax.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    ax.set_title(f\"h2 pairs movement: step {step_a} → {step_b} (top {top_k})\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.axhline(0, color='lightgray', linewidth=1)\n",
    "    ax.axvline(0, color='lightgray', linewidth=1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Also provide separate scatter plots for the same 5 pairs at each step (optional clarity)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10., 5.), constrained_layout=True)\n",
    "    ax1.set_title(f\"h2 pairs at step {step_a}\")\n",
    "    ax2.set_title(f\"h2 pairs at step {step_b}\")\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        x1, y1 = coords_b[idx]\n",
    "        ax1.scatter(x0, y0, s=25, color='k')\n",
    "        ax1.text(x0, y0, la[idx], fontsize=9, ha='left', va='center')\n",
    "        ax2.scatter(x1, y1, s=25, color='k')\n",
    "        ax2.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    for axx in (ax1, ax2):\n",
    "        axx.axhline(0, color='lightgray', linewidth=1)\n",
    "        axx.axvline(0, color='lightgray', linewidth=1)\n",
    "        axx.grid(True, alpha=0.2)\n",
    "        axx.set_xlabel(\"PC1\")\n",
    "        axx.set_ylabel(\"PC2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0367043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# h1 pair changes between steps 1500 and 1950: show top-5 moving pairs with arrows\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare h2 data (mean across seeds)\n",
    "if isinstance(results, tuple):\n",
    "    results = results[1]\n",
    "train_dict = results.get(\"train\", {}) if isinstance(results, dict) else {}\n",
    "if \"h1s\" not in train_dict or train_dict[\"h1s\"] is None:\n",
    "    print(\"No h1s found in results['train'].\")\n",
    "else:\n",
    "    H2 = train_dict[\"h1s\"]  # (seeds_n, time_steps, items_n, h2_size)\n",
    "    H2_avg = np.nan_to_num(H2.mean(axis=0), nan=0.0, posinf=0.0, neginf=0.0)  # (time_steps, items_n, h2_size)\n",
    "    time_steps, items_n_inferred, h2_size = H2_avg.shape\n",
    "\n",
    "    # Steps of interest (clip to available range)\n",
    "    step_a = min(1500, time_steps - 1)\n",
    "    step_b = min(1950, time_steps - 1)\n",
    "\n",
    "    # Build pair feature vectors by concatenation for all ordered pairs i != j\n",
    "    def build_pairs(X_items):\n",
    "        vecs, labels, pairs = [], [], []\n",
    "        for i in range(items_n_inferred):\n",
    "            for j in range(items_n_inferred):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                v = np.concatenate([X_items[i], X_items[j]], axis=0)\n",
    "                vecs.append(v)\n",
    "                labels.append(f\"({i},{j})\")\n",
    "                pairs.append((i, j))\n",
    "        V = np.asarray(vecs)\n",
    "        V = V - V.mean(axis=0, keepdims=True)\n",
    "        return V, labels, pairs\n",
    "\n",
    "    Xa, la, pa = build_pairs(H2_avg[step_a])\n",
    "    Xb, lb, pb = build_pairs(H2_avg[step_b])\n",
    "\n",
    "    # Fit PCA on the union to get a common 2D space\n",
    "    X_union = np.vstack([Xa, Xb])\n",
    "    if np.allclose(X_union.var(), 0.0):\n",
    "        theta = np.linspace(0, 2*np.pi, Xa.shape[0], endpoint=False)\n",
    "        coords_a = np.stack([np.cos(theta), np.sin(theta)], axis=1)\n",
    "        coords_b = coords_a.copy()\n",
    "    else:\n",
    "        pca = PCA(n_components=2, random_state=mds_seed if 'mds_seed' in globals() else 0)\n",
    "        coords_union = pca.fit_transform(X_union)\n",
    "        coords_a = coords_union[:Xa.shape[0]]\n",
    "        coords_b = coords_union[Xa.shape[0]:]\n",
    "\n",
    "    # Compute displacements per pair (match by index; la and lb have same ordering)\n",
    "    deltas = coords_b - coords_a\n",
    "    move_norm = np.linalg.norm(deltas, axis=1)\n",
    "\n",
    "    # Select top-5 moving pairs\n",
    "    top_k = 10\n",
    "    top_idx = np.argsort(-move_norm)[:top_k]\n",
    "\n",
    "    # Plot arrows for the top pairs\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    ax = plt.gca()\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        dx, dy = deltas[idx]\n",
    "        x1, y1 = x0 + dx, y0 + dy\n",
    "        ax.arrow(x0, y0, dx, dy, head_width=0.03, head_length=0.05, length_includes_head=True,\n",
    "                 fc='C0', ec='C0', alpha=0.9)\n",
    "        ax.scatter([x0, x1], [y0, y1], s=20, color=['C1','C2'], zorder=3)\n",
    "        ax.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    ax.set_title(f\"h2 pairs movement: step {step_a} → {step_b} (top {top_k})\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.axhline(0, color='lightgray', linewidth=1)\n",
    "    ax.axvline(0, color='lightgray', linewidth=1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Also provide separate scatter plots for the same 5 pairs at each step (optional clarity)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10., 5.), constrained_layout=True)\n",
    "    ax1.set_title(f\"h2 pairs at step {step_a}\")\n",
    "    ax2.set_title(f\"h2 pairs at step {step_b}\")\n",
    "    for idx in top_idx:\n",
    "        x0, y0 = coords_a[idx]\n",
    "        x1, y1 = coords_b[idx]\n",
    "        ax1.scatter(x0, y0, s=25, color='k')\n",
    "        ax1.text(x0, y0, la[idx], fontsize=9, ha='left', va='center')\n",
    "        ax2.scatter(x1, y1, s=25, color='k')\n",
    "        ax2.text(x1, y1, la[idx], fontsize=9, ha='left', va='center')\n",
    "    for axx in (ax1, ax2):\n",
    "        axx.axhline(0, color='lightgray', linewidth=1)\n",
    "        axx.axvline(0, color='lightgray', linewidth=1)\n",
    "        axx.grid(True, alpha=0.2)\n",
    "        axx.set_xlabel(\"PC1\")\n",
    "        axx.set_ylabel(\"PC2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip conjunctive_lazy_rich_h1s.csv\n",
    "!gzip conjunctive_lazy_rich_h2s.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env (tiexp)",
   "language": "python",
   "name": "tiexp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
